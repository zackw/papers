\documentclass{zarticle}
\addbibresource{proposal.bib}
\addbibresource{../fp-review-full/fp-review.bib}
\definecolor{todocolor}{RGB}{127,0,0}
\def\todo#1{{\color{todocolor}\bfseries [#1]}}
\def\needcite#1{\todo{cite: #1}}
\begin{document}

\title{Thesis Proposal --- DRAFT}
\author{Zachary Weinberg}
\maketitle

\begin{abstract}
Traffic analysis is specifically excluded from the standard threat
model for a secure channel.  We claim that this has become
unrealistic, and that failure to include traffic analysis in the
threat model leads to exploitable vulnerabilities “in the wild.”  We
propose to study these vulnerabilities and develop countermeasures.
\end{abstract}

\section{Background}

\emph{Traffic analysis} is the craft of extracting information from a
secure channel, using whatever data is still observable despite
encryption. Often this is metadata, such as the source, destination,
establishment time, and duration of the connection, and the protocol
in use.  In a typical modern packet-switched network, it is also
possible to observe when each individual packet is sent and the size
of its encrypted payload, and to perform TCP stream reassembly.  The
adversary may also be able to correlate observations of network
traffic at more than one point in the topology, or to correlate
network events with other events.  For instance, if the adversary
observes a large client-server upload to
\textsf{commons.wikimedia.org} at the same time as a new image appears
on that site, they may deduce that the image was uploaded by the
client under observation.

Traffic analysis has been practiced since the Great Game of the 19th
century, if not longer, but came into its own with the use of radio
for military communications in World War I.  French intelligence was
able to locate German encampments via radio direction finding, and
identify transmissions as originating from HQ, infantry, or cavalry
divisions because each produced a different volume and pattern of
traffic.  In the Pacific theater of World War II, both sides were able
to make educated guesses at upcoming naval maneuvers from radio
traffic volume, changes in callsigns, time correlation of signals, and
so on.~\cite{kahn1967codebreakers}

Nowadays, traffic analysis is more often applied to messages on a
network of some sort.  Felten~\cite{felten2013decl} observes that the
mere fact of a call placed to certain special-purpose telephone
numbers reveals “basic and often sensitive” information about the
caller.  There are dedicated support hotlines for survivors of
domestic violence, people considering suicide, and sufferers from
various forms of addiction; there are also lines dedicated to
anonymously reporting government misconduct, tax fraud, illegal
firearms, and other crimes.  If phone call records are analyzed over
time, they reveal the “social graph” and can even indicate the nature
of personal connections; Felten observes that regular late-night phone
calls suggest a long-distance intimate relationship.  The “dragnet”
surveillance programs (PRISM, X-KEYSCORE, etc.)\ recently revealed to
be conducted by the United States' intelligence
apparatus~\cite{greenwald2013nsa} rely on traffic analysis as a
first-stage filter: phone numbers registered outside the USA, calls to
people known to have called people on various “watch lists,” apparent
use of encryption, etc.\ all trigger more aggressive data collection.

\subsection{Unlinkability}

Because it is so easy to extract information from the source and
destination of encrypted communications, protocols have been developed
specifically to conceal this information.  In addition to all the
properties of a secure channel, an \emph{unlinkable} channel protocol
seeks to ensure that no eavesdropper at a single point in the network
can learn \emph{both endpoints} of the channel.  That is, no
eavesdropper can tell that Alice is talking to Bob over an unlinkable
channel, but eavesdroppers can still tell that Alice and Bob are
talking to \emph{someone}.  (This definition is due to
Pfitzmann~\cite{pfitzmann2010terminology}.  Whether or not active
attackers are considered depends on the writer.)

The standard technique for providing unlinkability, invented by
Chaum~\cite{chaum1981mix}, is to pass each message through one or more
“relays,” layering encryption so that each relay only knows the
preceding and next link in the chain. One relay is enough against most
eavesdroppers, but provides no protection if the relay itself is
malicious; operators of popular single-hop “proxy services” or
“anonymous remailers” may, and indeed have, come under coercion to
expose their users.~\cite{newman1996church,
  singel2007hushmail,ackerman2013lavabit} Similarly, a two-hop chain
is vulnerable to a pair of cooperating malicious relays.  In
principle, a three-hop chain is secure unless the adversary controls a
significant fraction of the network, and more hops only add
overhead~\cite{wright2002analysis, wright2003defending}.

Chaum's original design was geared for email, so it imposed
significant delays at each relay in order to conceal the exact time of
each message.  This delay is unacceptable for interactive applications
(such as online chat and Web browsing), and the current generation of
“low latency” unlinkable services, such as
Tor~\cite{dingledine2004tor}, deliberately omit it.  This opens an
avenue for “intersection” attacks by adversaries who \emph{do} control
a significant fraction of the network, or who can observe traffic at
many points in the network; these are formally out of scope, but have
still received a good deal of attention in the literature,
e.g.~\cite{danezis2003statistical, danezis2004continuous,
  danezis2005statistical, danezis2007twosided, murdoch2007sampled,
  shmatikov2006timing, overlier2006locating} Defenses against these
attacks are still being researched, although a few simple tactics have
been deployed, such as “guard nodes”~\cite{overlier2006locating}: the
attacks are most effective if a malicious node is directly connected
to a client, so by always beginning one's chains with the same node
for an extended period, one avoids having any of one's traffic be
exposed to a malicious node (assuming that the initial choice is
sound).

\subsection{Confidentiality of Message Length}

The standard mathematical definition of “confidentiality” assumes that
all messages are the same length.  In practice, this is not true, and
as recently demonstrated~\cite{duong2012crime,gluck2013breach}, that
permits serious active attacks on protocols that compress data before
encryption.  But even a pure eavesdropper can learn something from the
size of a message.

HTTP (over any secure channel, unlinkable or not) is particularly
vulnerable to this attack because it exhibits strict turn-taking
behavior: a single TCP connection can be reused for several
query-response pairs, but the client cannot begin a new query until it
has received the complete response to its previous query, nor can the
server begin a response before the query is fully
received.\footnote{HTTP 1.1 includes a \emph{pipelining} mechanism
  that lifts this restriction, but unlike the rest of protocol 1.1,
  pipelining has never seen wide adoption.  In the past few years,
  radical revisions of HTTP which also lift this restriction, such as
  Google's SPDY, have been proposed and adopted to some extent; we are
  not aware of any research into how this changes the situation for an
  eavesdropper.}  Thus, the approximate length of each request and its
corresponding response is apparent to an eavesdropper capable of TCP
stream reconstruction.  Furthermore, most web pages incorporate by
reference a set of “resources,” such as images, style sheets, and
scripts, which must be loaded in separate HTTP transactions (unless
cached); thus two pages of approximately the same length may be
distinguishable because they refer to different resource sets.

We will give three examples of concrete breaches of confidentiality
via message length analysis.  All these attacks assume an adversary
eavesdropping directly, or perhaps at one or two hops' remove, upon a
specific, known target user who is browsing the Web.  The simplest
attack observes that the vast majority of Facebook and Google profile
images have a unique size.  Detecting the HTTPS request for this
image, and measuring its length, thus permits an eavesdropper to
associate a particular social-network identity with a particular IP
address.~\cite{herrmann2012analyzing, pironti2012identifying} A more
sophisticated “state tracing” attack seeks to reconstruct a sequence
of page loads from the lengths of HTTPS traffic bursts.  Since the
structure of a public website is known to the attacker, this reveals
how the targeted user is interacting with the site.  Applied to a
search engine, this can reveal queries~\cite{castelluccia2010private};
applied to a tax-preparation website, this can reveal the target's
approximate income, marital status, and other such highly confidential
data.~\cite{zhang2010sidebuster} Finally, if the targeted user is
browsing the Web via an unlinkable channel, an eavesdropper may be
able to identify the \emph{site} that is being accessed from the
pattern of response sizes.  This possibility has received substantial
attention in the literature, e.g.~\cite{cai2012touching,
  cheng1998traffic, coull2007web, dyer2012peekaboo} but it is unclear
to us whether it can be scaled from these laboratory experiments
(typically covering at most a few thousand sites' front pages) to the
entire Web.

The basic defense against an eavesdropper extracting information from
message length is, of course, to pad messages.  However, if done
poorly, padding won't help at all; for instance, padding each packet
up to the nearest multiple of 512 or 1024 bytes adds only a trivial
amount of uncertainty to the length of a much longer message.  If done
more thoughtfully, padding can help, but often at unacceptable cost in
transmission time.  The best known techniques rely on
application-layer knowledge of the full dataset whose members are to
be indistinguishable (or clustered into indistinguishable
groups).~\cite{chen2010side, mather2012pinpointing,
  backes2013sidechannel} While this may be practical for
e.g.\ Facebook to apply to all its profile photos, it is no “magic
bullet” that will fix all the sites on the net.  (Compare the notion
of ideal steganography~\cite{hopper2009provably}; it works great
provided you know the true distribution of covertexts, which in
practice can't even be modeled well.)

\subsection{Unobservability}

An \emph{unobservable} protocol would have the property that no
eavesdropper could tell that anyone was talking at all.  This is a
very strong property, approached by some physical radio
encodings~\cite{pickholtz1982theory,yu2007dsss} but not actually
achieved when there is any cleartext structure to the data
transmitted~\cite{jia2013blind}, or if all transmitters don't
continuously broadcast at maximum gain (which is obviously
undesirable).  In packet networks, where it is not \emph{possible} for
all nodes to transmit continuously all the time, unobservability is
generally not even considered.  A weaker definition is for no
eavesdropper to be able to tell whether any node is the
\emph{originator} of a message.  Even this weaker notion, at least to
date, requires too much “dummy” traffic to be considered practical,
and the topic has been neglected.

Unobservability, in the strong sense, is the only theoretically-sound
defense against an arbitrarily powerful traffic-analytic attacker,
so we suggest that the topic should be revisited, with specific
attention to whether the weak definition will suffice against
realistic attackers, and how much of that “dummy” traffic can be made
to be useful.

\section{Research Questions}

We propose to investigate the extent to which traffic analysis really
does destroy confidentiality, and how practical it is to do something
about it.  Concretely, we plan to investigate the following three
questions.

\paragraph{To what extent does size-and-timing analysis destroy
  confidentiality?}  As discussed above, this has been studied to some
extent, but mostly in “laboratory” settings.  The most concrete
results are all to do with single sites: learning search queries, user
profile images, answers to questions on a question-and-answer site
(e.g.\ for tax preparation).  Except for profile images, these involve
active interaction with the site.  One avenue for further exploration
would be to study whether “passive” browsing on a large, complicated
site reveals interesting things about the user.  Wikipedia has lots of
innocuous articles, a fair number that one might be embarrassed to
admit one has read, and a handful that are so politically or
culturally sensitive that they cannot even be referred to in some
circles.  If these pages (or clusters of pages) can be distinguished
by content length or detailed packet timings, that enables the
adversary to learn something interesting.  Similarly, social-blogging
sites such as Tumblr carry an enormous range of content, with op-ed
columns, art, kittens, personal journals, and pornography all jumbled
up together.  Facebook is complicated enough that it is likely to
present a radically different network traffic profile for different
logged-in users.  (No study to date has looked at logged-in users in
detail.)

When an unlinkable channel is used, the adversary needs to identify
the site being accessed before they can do any of the above, and it
may be enough for them to know that a particular site is being
accessed.  (Existing “Internet filtering” programs, however motivated,
rely on blocking of entire websites as their principal
tactic.~\cite{aase2012whiskey}) While this too has been studied, it
has not been studied in what we would consider a realistic
\emph{field} setting.  Specifically, most of the literature attempts
to discriminate the front pages of a few hundred servers, and examines
only one traffic source, whereas country-scale “filtering” applies to
millions of clients and and must consider \emph{all} servers worldwide as
potential sources of undesirable material.  The number of servers that
actually do carry undesirable material is much smaller, and only some
of those come to the notice of the censor, but the list still
potentially extends to tens of thousands of addresses.  Furthermore,
front pages, which express the site's corporate identity, are
plausibly more different from each other than internal pages, which
are often a stock set of “chrome” wrapped around a blob of text.

As a first step toward a more realistic field study of site
identification, we propose to assess a much larger sample of sites,
drawing on sources beyond the Alexa top $N$'s front pages, such as:

\begin{itemize}
\item URL shorteners contain links to material that people thought
  worth publicizing, indiscriminately as to topic.  (It will probably
  be necessary to manually categorize the material.)
\item “Scraping” sites operated for the specific purpose of sharing
  links of interest, often with categorization, such as Digg, Reddit,
  Metafilter, and (some areas of) 4chan.
\item Special-purpose directories of sites, such as the Hidden Wiki,
  which lists sites operated as Tor hidden services (hidden services
  conceal the identity of the site operator, so these sites are more
  likely than the average to carry controversial content).
\item Programmatically traversing internal and external links on sites
  revealed by all of the above.
\item Exhaustive “crawls” of sites of particular interest, e.g.\ Wikipedia.
\end{itemize}

\paragraph{How practical is it to conceal within-site details via
  application layer countermeasures?} While there are concrete
proposals for semi-automated detection and masking of “side channels”
(including resource length) in Web
applications~\cite{backes2013sidechannel, chapman2011automated,
  zhang2010sidebuster}, to our knowledge no one has attempted to
deploy them on a large public website.  We are considering a detailed
case study of such a deployment, probably on Wikipedia (whose
developers have indicated interest in aggressive anti-surveillance
measures).

It would first be necessary to produce a threat model, enumerating all
potentially-sensitive pieces of information that might be revealed via
traffic analysis (not just via message length), such as:

\begin{itemize}
\item Which page is being visited?
\item Which \emph{category} of page (main, Talk:, User:, etc; topic
  clusters) is being visited?
\item What language? (Currently exposed via server hostname, but might
  not be forever.)
\item Is this IP address making edits?
\item Is this IP address accessing WP as a logged-in user? Do they
  have administrative privileges?
\end{itemize}

We would then explore ways to conceal this information.  Some things
can be done immediately, such as deployment of SPDY, which (at least
theoretically) eliminates the strict turn-taking pattern of HTTP and
therefore should make the pattern of subresource loads for each page
less obvious; with some tuning, it might be possible to eliminate that
pattern entirely.  Other simple changes to server configuration should
also help, such as making sure that the HTTP request and response
headers do not change size upon login.  The main project, though,
would be (with the assistance of the Wikimedia organization) to
implement, test, and deploy a scheme for automated dataset-aware
padding of encyclopedia pages.  This would not necessarily make
\emph{all} of the pages indistinguishable, but it would ensure that
there were enough possibilities for each page load that learning
anything interesting from each event was infeasible.  It could also
make sense to precache resources that might or might not be needed,
such as scripts and style sheets for the page editor, in order to
disguise whether the editor was ever used.  (Note however that it may
be infeasible to hide the fact that an IP address has just made an
edit, since this intrinsically involves transmitting the new text to
the server, so the HTTP request cannot help but be larger than
normal.)

\paragraph{Can we bake traffic-analysis resistance into the next
generation of Internet protocols?}  It is probably not feasible to
eliminate traffic analytic attacks on the present generation of
Internet protocols.  There are too many ways in which critical state
is exposed to the network, and too many basic mechanisms can barely
handle the amount of security that's already been bolted onto them.
The next-generation protocols currently being designed
(e.g. XIA~\cite{xia2013xia}) treat security as a principal design
goal, and we expect it will be easier to explore \emph{generic}
(without application-layer support) traffic analysis resistance in
this context.

This is an open ended exploratory project.  We will limit its scope to
document publication and retrieval, primarily because this application
is more latency-tolerant than anything related to real-time
conversations (chat, VoIP, etc.) and secondarily because this is the
principal battlefield for online censorship and surveillance.  Note
that we are not giving up much; the existing capabilities of the “Web
platform” already permit construction of applications with nearly all
logic on the client side, the server only acting as a distribution
point for static data, and planned additional features will only make
this easier.  In fact, moving logic to the client automatically
reduces users' exposure to traffic analysis: for instance, it would
not be nearly as easy to extract information from a tax-preparation
application if its logic was transmitted to the client all at once,
and only the finished paperwork was sent back, instead of a long
sequence of questions and answers being sent over the network one at a
time.

Publish-retrieve systems with some degree of anonymity, censorship
resistance, and/or surveillance resistance have been proposed before;
designs of note include Free Haven~\cite{dingledine2000freehaven},
Tangler~\cite{waldman2001tangler}, and the Eternity
Service~\cite{anderson1996eternity, benes2001strongeternity}. So far
none has achieved wide deployment, and some have been found to have
serious flaws~\cite{kugler2003gnunet}.  Distributed caches can reveal
retrieval history; even when caches are not supposed to be able to
learn decryption keys for the content they hold, as
in~\textcite{serjantov2002anonymizing}, keyword searches for
undesirable content can finger caches that hold it, and thus the users
that might have retrieved it.

However, the basic notion of distributed storage for self-validating,
content-addressed material seems sound, and presents a number of
desirable affordances: The threat models and designs in this space
typically do include explicit consideration of retrieval anonymity and
censorship and surveillance resistance.  Opportunistic retrieval and
caching (or validation) offer a degree of unobservability, by offering
a node plausible deniability as to whether any given data object was
retrieved by a human.  Tangler in particular suggests a way toward
genuinely oblivious caching, i.e.\ the presence of a particular
encrypted blob in a cache need not reveal that a particular document
has ever been retrieved through that cache (even if the adversary
knows \emph{some} of the documents that that blob can produce).
Finally, although it is not directly related to the question of
traffic analysis, these systems all build on a notion of signing keys
for content, which could supersede the Web's creaky scheme of
certificate authorities (as long as some method can be found to
name these keys which isn't inscrutable to humans).

\section{Social Considerations}

We are also concerned with what the “new” Web, providing (let us
suppose) true anonymity for readers, strong pseudonymity for authors,
and strong guarantees that content once published cannot be removed,
will look like as a social phenomenon.  In particular, the
\emph{present} online culture has tendencies toward groupthink, abuse
of outsiders, and general hostility toward anyone who looks like a
tempting target. One might argue that “baking in” masks for everyone
to hide behind will only make all of this worse.

We first observe that no technical measure can hope to distinguish
“legitimate” political speech from harassment of private citizens from
straight-up trolling with certainty.  Merely deciding whether or not a
document expresses a negative opinion, in the general case, is an
AI-complete problem.\footnote{By analogy to NP-completeness,
  “AI-complete” refers to problems that, if solved in software, would
  imply that a human-equivalent artificial intelligence had been
  created.}  Creating a network that would somehow prevent online
abuse without also chilling political speech acts is \emph{harder}
than AI-complete: even if we had the necessary AI, who would get to
decide what policy it would apply?  Consider that one person's idea of
a legitimate political cartoon may be another person's idea of
\textit{lèse-majesté} or blasphemy.

However, we do not think the situation is hopeless.  Spam filters are
not perfect, but do a good enough job to be useful, especially in
conjunction with human moderation.  The same statistical and heuristic
techniques could be applied to filter out trolls and bullies.
Further, content filters and moderation are not intrinsically
unethical; what is problematic from a freedom-of-speech perspective is
when they are imposed on individuals who would prefer not to have
them, or when they are used to punish the authors of controversial
content.  We suggest that the application layer could offer different
views of the same site to all visitors, allowing them to pick their
desired level of curation.  Exactly how this would work is beyond the
scope of the present proposal, but we are interested in developing it
further, down the road.

\printbibliography

\end{document}
