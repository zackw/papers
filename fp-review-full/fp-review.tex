\documentclass{zarticle}
\addbibresource{fp-review.bib}
\definecolor{todocolor}{RGB}{127,0,0}
\def\todo#1{{\color{todocolor}\bfseries [#1]}}
\def\needcite#1{\todo{cite: #1}}
\begin{document}

\title{Unmasking Web Users and Activities:
  A review of the literature on traffic analysis of the modern Internet}
\author{Zachary Weinberg}
\date{\today}
\maketitle

\begin{abstract}
Abstract abstract, abstract abstract abstract.
\end{abstract}

\section{Introduction}

It is often infeasible to conceal a message's sender, recipient,
length, time of transmission, or its relationship to other events.
\emph{Traffic analysis} is the craft of deducing information about the
content of a secret message from these observables.  Classically, one
might deduce that a naval vessel has received new orders if it
abruptly changes course shortly after an encrypted radio broadcast.
\todo{More history?}

A protocol which provides cryptographic confidentiality, integrity,
and authenticity on top of a best-effort, cleartext infrastructure is
generally referred to as a “secure channel,” but traffic analysis can
extract information from most secure-channel protocols used on the
modern Internet, damaging to some extent the abstract promise of
confidentiality.  In this paper we will review the state of the art in
traffic analysis specifically of HTTP (World Wide Web) transactions
relayed through proxies which are intended to guarantee the anonymity
of their users.

\subsection{Levels of Anonymity}

“Anonymity” is an overloaded term.  Following
Pfitzmann~\cite{pfitzmann2010terminology}, in this paper we will
distinguish three levels of anonymity protection.  \emph{Ordinary}
traffic is carried over a secure-channel protocol (such as TLS or
SSH), so its content is concealed, but it enjoys no protection from
traffic analysis.  Most importantly, the sender and recipient of each
message are evident to any observer (i.e.\ the IP source and
destination addresses). The information that Alice did at some point
talk to Bob has sometimes been enough to send Alice to
prison!\footnote{As an extreme example, the Soviet Union notoriously
  criminalized “contacts leading to suspicion of
  espionage.”~\cite{solzh74gulag:svpsh}} An eavesdropper can also
learn the amount of application data each packet carries, its position
within a TCP stream, and other metainformation, such as any TCP
options that are present.

\emph{Unlinkable} traffic enjoys the additional guarantee that an
adversary will not be able to determine that a particular sender was
in communication with a particular recipient.  This is normally
achieved by passing the traffic through one or more relays, such that
each knows only the previous and next hop; a secure channel is still
established end-to-end, so the relays themselves cannot observe
message contents.  As long as an eavesdropper cannot correlate traffic
near Bob with traffic near Alice, the most they can learn is that both
Alice and Bob talked to \emph{someone}.  Unlinkability is the level of
protection provided by most anonymity services in wide use on the
public Internet.

Finally, \emph{unobservable} traffic is indistinguishable from
background chatter, so an adversary cannot be sure that any given
station is talking to \emph{anyone}.  Unobservability represents an
ideal; although there are a few proposals, no unobservable protocol
has been made practical.  \todo{expand}

\subsection{Attacking Unlinkability}

There are a wide variety of threat models in the literature for
traffic-analytic attackers on unlinkable channels.  The attacker may
be located at several different places within the network, and may
have one, two, or several listening posts.  It may be strictly an
eavesdropper, or may be allowed to generate its own traffic, or may
even operate its own malicious relays.  It may be able to communicate
overtly with its victims, for instance by enticing them to visit an
attacker-controlled website; this is more powerful than it might
sound, since a malicious website can execute code (in a restricted,
but not at all foolproof, environment) on its victims'
computers.~\cite{barth2008securing} However, “Byzantine” attacks, in
which the attacker interferes with the correct operation of the
unlinkable channel protocol, are considered a separate class of
threat.  The attacker's goals are even more diverse, ranging from
the deduction of encrypted channel content to tracing messages through
the relay chain to identifying communicants. \todo{sprinkle cites in
  this paragraph}

Unlinkable protocols don't generally conceal the size or the timing of
messages.  There are exceptions: Chaum's original mix design (intended
for email) did include substantial delays at each relay in order to
conceal timing, but this is not considered acceptable for modern
interactive protocols such as HTTP: current-generation mix networks,
such as Tor~\cite{dingledine2004tor}, treat their \emph{low latency}
as a valuable feature.  Unlinkable channels may or may not disguise
the TCP session to which each packet belongs. \todo{talk about how
  this leads to attacks}

When an unlinkable protocol uses only one relay, the relay is
colloquially known as a “proxy server,” and is a point of
vulnerability, since it knows the source and destination of every
message.  Even if run with the best of intentions, proxy servers may
come under coercion to expose that information.\footnote{As a less
  extreme example, the email relay \textsf{anon.penet.fi} shut down
  under ongoing legal pressure to unmask its
  users.~\cite{newman1996church}} A \emph{mix
  network}~\cite{chaum1981mix} replaces the single relay with a chain
of relays, each of which knows only the previous and next hop;
subverting any one relay thus reveals no more than eavesdropping would
have.  In practice, three relays are used, because a two-relay chain
is insufficient to defend against attacks by pairs of colluding
malicious relays, but a chain of four or more relays offers only
marginal additional security at substantial cost in
latency.~\cite{wright2002analysis,wright2003defending}  Some of the
research we review studied the behavior of proxy servers, while other
papers examined mix networks.  We will refer to both as unlinkable
protocols except where the difference matters.

\subsection{Website Fingerprinting}

This review focuses on two classes of traffic analysis attacks, both
of which are referred to in the literature as “website
fingerprinting.”  Both feature an attacker eavesdropping on a
victim-user at or near the victim's location in the network;
specifically, the attacker is assumed to be able to observe traffic
between the victim's computer and the first relay in the unlinkable
channel.  Therefore, the attacker already knows something about the
victim, and their goal is to learn something about what they are using
the unlinkable channel for.  (One result discussed
below~\cite{gong2011remote} suggests the possibility of fingerprinting
from afar by flooding the victim with ICMP “ping” messages, but the
traffic so observed is still the traffic between the victim's computer
and the first relay.)  In one class of fingerprinting attacks, the
attacker simply seeks to determine which Web sites the victim is
browsing via the unlinkable channel.  In the other class, the attacker
assumes that the victim is browsing a particular website, and attempts
to learn something about what they're doing there.

\todo{Machine learning, techniques, cost}

\section{Identifying Websites}

The Web being the most common application layer use of the modern
Internet, it is a safe bet that the victim is using it; however, for
the same reason, the number of possible sites that the victim might be
browsing is enormous.  Most published attacks of this form limit
themselves to a relatively small number of sites, usually the most
popular 100 to 10,000, either based on local traffic sampling, or the
global rankings published by Alexa.  These are not terribly realistic
when one considers \emph{why} the attacker might want to know which
sites their victim browses; we will come back to this point later.

\subsection{Laboratory-scale experiments}

\subsection{Larger experiments}

\section{Identifying Activity}

\subsection{Unmasking Users of Social Networks}

The data stream that these sites produce when loaded is predictable,
and more importantly, has predictable variation that depends on, for
instance, the file size of the victim's profile photo, which may well
uniquely identify the victim.  (The attacker knows the address of the
victim's computer, but that doesn't mean they already know the
victim's Facebook account ID.)  These attacks are more realistic, but
since they only look at data coming from one site, application-level
countermeasures are feasible.

\section{Countermeasures}

\todo{application-layer padding, transport-layer padding, dummy
  traffic, full-on steganography}

\section{Related attacks}

\todo{tracing messages through a mix, etc}

\section{Discussion}

\printbibliography
\end{document}
