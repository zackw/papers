* Deep background

** The secure channel and its properties

confidentiality, integrity, authenticity -- precise definitions of
each; what they _don't_ cover

** Low-latency mix networks and their properties

built on top of secure channels; drop-in replacement for an end-to-end
secure channel, offers additional guarantee -- unlinkability of
actions to actors

what still isn't covered

some of the research we review considers a single "proxy server"
instead of a mix network; provides the same unlinkability guarantee to
an eavesdropper at a single point in the network, but no protection
if the proxy server itself is malicious.

** Traffic analysis
*** abilities of a traffic-analysis attacker

information available: message size and timing; possibly one endpoint
of a communication of interest

in a packet-switched network, individual packet sizes and timings may
be used directly, or may be aggregated to estimate size and timing of
a higher-level message (one HTTP transaction, for instance)

limited to eavesdropping on victim's messages, but may also be allowed
to send and receive messages of their own; may or may not be able to
communicate overtly with the victim (e.g. by enticing the victim to
visit a web site operated by the attacker) or send probe messages
(e.g. 'ping's) to specific computers

*** goals of a traffic-analytic attacker

determine a message's path through a mix network; not normally an end
in itself

knowing one endpoint of a message, determine other endpoint

learn something about the content of a message or sequence of messages
(mention VoIP decoding from VBR packet sizes)

*** Website fingerprinting

specific focus of this paper: two classes of traffic analysis attacks.
both referred to as "website fingerprinting".  both feature an
attacker eavesdropping on a victim-user at or near their location in
the network; in particular, can observe traffic between the victim's
computer and the first mix-network relay.  (one result suggests
possibility of eavesdropping from afar via ping flooding, but the
traffic under observation is still victim to first relay.)

attacker may seek to determine identity of website being visited by
victim (Web being most common application layer use of modern Net) or,
on the assumption that victim is visiting a known website (small
handful of very popular sites account for the bulk of modern Web
traffic), determine what he/she is doing there.

things the attacker can do with this information

* Techniques for fingerprinting

break down by machine-learning algo, data input

* Determining website identity

(size of 'world', realism of traffic analyzed)

** On Web Browsing Privacy in Anonymized NetFlows [coull2007web]

focuses on extracting features from offline traces of network
activity, particularly anonymized traces released by research
projects.  strictly less info than would be available to an online
attacker; on the other hand, offline processing means more CPU grunt
available.  major divergence from eavesdropping on a proxy is
availability of "physical server" information (don't know what server
it is but you do know _how many_ servers have been contacted and that
they're not the same as other servers).

also look at realistic load-to-load changes in the content of a
webpage (due to caching or dynamic content). suggest using cumulative
load size as a third feature in addition to flow size and flow index
(within a larger sequence of downloads), partitioning by server IP and
then clustering 'physical' to 'logical' servers (to account for CDN IP
diversity).  kernel density estimates as similarity metric, input to
binary-tree Bayesian belief network for classification.

closed world test using a real browser with its cache enabled. trained
on alexa top 50, tested on top 100 (identify as one of top 50 or as
unknown site).  true detection rate of 50% overall (chance would be
2%); false detection 0.18%; point out that this is an unacceptably
large info leak for supposedly anonymized data release.  per-site TD
rate varies from 95% all the way down to 20%, correlated with dynamism
of the site (more dynamic = less reliable identification).  drill down
on this = critical factor for TD rate is variability in number of flows
generated for each page load.  conversely, sites that are very simple
get high TD rate but also high FD rate because there are lots of
situations where something can be mistaken for them.

"with regard to realistic threats to anonymized network data, these
results show that there are certain web pages whose behavior is so
unpredictable that they may be very difficult to detect in
practice. also, an attacker has little chance of accurately
identifying small, simple web pages with our techniques. complex web
pages containing large CDNs, on the other hand, may allow an attacker
to identify these pages within anonymized flow traces with low false
detection rates.  finally, we have found that an attacker must
consider the effects of locality on the training data..."

repeat experiment on traffic from a real wireless hub with similar
results (not clear what sites were in the training data).

* Determining activity on particular website

** Traffic Analysis of SSL Encrypted Web Browsing [heyning1998traffic]

using an exhaustive crawl of a particular public site, identify
exactly which page is being visited based on total size of data
transferred.  point out that nearly all HTML pages have a unique size,
and for those that aren't, disambiguation is possible based on what
images each page loads.  suggest using a Markov model to track users
from page to page.  brief discussion of possible countermeasures, not
analyzed in detail.  this is fairly old and some of what it says may
not apply to modern websites / browsers but it's interesting how
little the attack and defense techniques have changed.

* Defenses

** Application-layer padding

** Transport-layer padding

*** Traffic Morphing: An Efficient Defense Against Statistical Traffic Analysis [wright2009traffic]


Notion is to transform the packet sequence to be concealed so that it
mimics the statistics of another, innocuous packet sequence.  Define a
matrix operator that maps the probability mass function of the
original packet sequence to the pmf of the cover sequence, and use
mathematical optimization to minimize overhead.  Demonstrate
effectiveness versus naive [liberatore2006inferring] and
[wright2007language] and superiority to fixed-length padding (however,
see argument made by [dyer2012peek] re coarse features)

*** From ideality to practicality in statistical packet features masking [iacovazzi2012ideality]

Statistical model of information leaked by packet flows; define an
*ideal* mask as producing the same packet stream regardless of the
behavior of the input source, then relax that in two formal ways:
*impractical* vs *practical* masking (latter has no knowledge of the
future) and *full* vs *partial* masking (latter accepts some residual
leaked information in exchange for reduced overhead).  Mathematical
algorithm for partial, practical masking; requires training on
application behavior, but not modification of the application itself.

** Dummy traffic

* Related attacks

** Tracing messages through a mix network

*** Tor traffic analysis using Hidden Markov Models [zhioua2012ckthmm]

Markov model of Tor control protocol; uses inter-cell timing as
primary metric; can determine when circuits are created; with training
very close in time and space to monitored victim, can determine
identity of each hop in a new circuit.

attacker in same LAN as client, runs its own Tor sessions to gather
training data

in-practice reliability unclear

* Unsorted papers

** The Economics of Mass Surveillance and the Questionable Value of Anonymous Communications [danezis2006econ]

if anonymous users are divided into communicating cliques, how many
users must be compromised to reveal the membership of each clique?

instead of giving a number, analyze several strategies for picking
users (it's a social network, if you can find the people with the
highest out-degree, you will compromise more groups faster)

of course, attacker doesn't know out-degree a priori. traffic volume
turns out to be an excellent proxy for high out-degree; existing
mixnets don't even try to cloak it.  may not be the best practical
strategy but two obvious adaptive strategies do not beat it

large, open cliques are compromised first; a secret organization with
good tradecraft (cell structure, etc) can avoid detection until long
after substantial collateral damage (surveillance of innocents) has
been done

** A Large-Scale Study of the Evolution of Web Pages [fetterly2004pageevol]

