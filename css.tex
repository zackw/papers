\documentclass{acm_proc_article-sp}
\usepackage{url}
\usepackage{verbatim}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{booktabs}
\usepackage{upquote}
\hyphenation{brow-ser brow-sers tra-dit-ion-al ja-va-script Web-Kit}
\begin{document}
% fix some inappropriately large gaps - some of this must be after
% begin document
\itemsep 0pt
\partopsep 0pt
\topsep 0pt
\makeatletter
\def\paragraph{%
    \@startsection{paragraph}{4}{\z@}{\z@ \@plus \p@}{-5\p@}{\subsecfnt}}
\makeatother
\renewenvironment{itemize}{%
 \begin{list}{$\bullet$}
  {\setlength{\itemsep}{0pt}
   \setlength{\parsep}{3pt}
   \setlength{\topsep}{3pt}
   \setlength{\partopsep}{0pt}
   \setlength{\leftmargin}{1.5em}
   \setlength{\labelwidth}{1em}
   \setlength{\labelsep}{0.5em}}}
  {\end{list}}

\title{Protecting Browsers from Cross-Origin CSS Attacks}
\numberofauthors{4}
\author{
\alignauthor
Lin-Shung Huang\\
      \affaddr{Carnegie Mellon University}\\
      \affaddr{linshung.huang@sv.cmu.edu}
\alignauthor
Zack Weinberg\\
      \affaddr{Mozilla}\\
      \affaddr{zweinberg@mozilla.com}
\and
\alignauthor
Chris Evans\\
      \affaddr{Google}\\
      \affaddr{cevans@google.com}
\alignauthor
Collin Jackson\\
      \affaddr{Carnegie Mellon University}\\
      \affaddr{collin.jackson@sv.cmu.edu}
}

\newcommand{\todo}[1]{\textbf{[TODO: #1]}}

\maketitle
\begin{abstract}
Cross-origin CSS attacks use style sheet import to steal confidential
information from a victim website, hijacking a user's existing
authenticated session and bypassing cross-site defenses.  We show how
to conduct these attacks with any browser, even if JavaScript is
disabled, and propose client-side defenses that still allow the vast
majority of web sites to function normally. We have implemented and
deployed defenses in Firefox, Google Chrome, and Safari. Our defense
proposal has also been adopted in Opera.
\end{abstract}

\category{K.6.5}{Management of Computing and Information Systems}
                {Security and Protection}

\terms{Security}

\keywords{CSS, MIME, Same-Origin Policy}

\section{Introduction}

The World Wide Web was originally envisioned \cite{wwwproposal} as a
means to collate a wide variety of human-readable, static documents,
present them via a unified interface, and facilitate browsing through
them by searching or via inter-document references. It has grown into
a versatile platform for all kinds of computing tasks, progressively
gaining support for data entry, client-side scripting, and
application-specific network dialogues.  Web-hosted applications have
supplanted traditional desktop applications for almost everything that
requires network communication, and are becoming competitive in other
areas.  It is not an exaggeration to say that the Web is the
development platform of choice for new software.

The \emph{same-origin policy}~\cite{mozillasameorigin} is the basic
principle used to secure Web applications from each other.  Scripts
belonging to one website can only communicate with that site's
servers, and cannot access the contents of pages loaded from other
sites.  However, this compartmentalization applies only to scripts.
An HTML document can load any sort of content---images, style sheets,
nested documents and “plug-ins,” even scripts---from any site,
same-origin or not.  In theory, this is a safe, useful capability:
rarely-changing content like images may be hosted on servers dedicated
to that purpose; popular script libraries (jQuery, Prototype, etc) may
be shared among sites; pages may incorporate YouTube-hosted videos
instead of just referring to them.  Browsers apply the same-origin
policy even within what appears to the user to be one unified “page;”
for instance, scripts can only inspect the DOM tree for an
\texttt{IFRAME}'s nested document if that document came from the same
origin.

Cascading style sheets (CSS) are the third principal component of Web
documents; they define appearance, just as HTML defines content and
JavaScript behavior.  Of the three, CSS was invented last; proposals
for author control of style were circulated as early as
1993~\cite{css-history}, but the first complete specification dates to
1996~\cite{css1} and was not implemented in a widely-used browser till
1997~\cite{eich}.  The CSS specification is continually being
extended, and its original designers planned for this.  As long as new
CSS features conform to the \emph{forward-compatible parsing rules}
defined in \cite{syndata}, old browsers will skip over features they
do not implement, while continuing to honor instructions that they do
understand.  Web designers can thus build sites that take advantage of
the very latest CSS features but “degrade gracefully” and remain
usable with older browsers.  Unfortunately, the forward-compatible
parsing rules are so permissive that they can find valid CSS
constructs in an input stream that was not intended to be CSS at all;
for instance, in an HTML document.

This leads to a security hole, first described (to our knowledge) in
2002 \cite{cssxss02} and rediscovered at least twice since then
\cite{cssxss05,cssxss08}.  If a malicious site can inject chosen
strings into a target webpage (whose structure, but not specific
contents, are known) and then load that page as a style sheet, it can
extract information by examining what the CSS parser makes of this
“sheet.”  even if the target page cannot be retrieved
without presenting login credentials, because the browser will present
any credentials (e.g.\ HTTP cookies) it has stored for the target
server when it does the load.  However, to date, all published attacks
of this type have required JavaScript, and most have been specific to
Internet Explorer.

In this paper, we present a general form of the attack that can be
made to work in any browser that supports CSS, even if JavaScript is
disabled or unsupported.  We then propose and implement modifications
to browser handling of CSS that completely block the attack, as long
as the victimized web site does not make certain errors (discussed
below).  Our modifications have no negative side effects for most
websites, and have been adopted by Firefox, Google Chrome, Safari, and
Opera.

\paragraph{Organization}
The rest of this paper is organized as follows. Section~\ref{sec:threatmodel} presents a threat model for cross-origin CSS attacks.
Section~\ref{sec:attacks} describes the attack in detail. Section~\ref{sec:defenses} proposes and evaluates defenses.
Section~\ref{sec:relatedwork} surveys related work.
Section~\ref{sec:conclusion} concludes.

\section{Threat Model} \label{sec:threatmodel}

The threat model for cross-origin data theft with CSS is essentially
the same as the threat model for other cross-origin attacks.  A
\emph{web attacker}~\cite{jackson09thesis} is a malicious principal
who owns a domain name and operates a web server.  The web attacker's
goal is to steal data from another web site (the \emph{target}) that
should only be revealed to a particular user (the \emph{victim}) and
not to the attacker.  Alternatively, the goal may be to forge requests
to the target site using the victim's credentials.  Cross-origin CSS
attacks directly facilitate the first goal, and may indirectly
facilitate the second goal by giving the attacker access to session
credentials.

The web attacker can send and receive arbitrary network traffic, but
only from its own servers.  It cannot modify, or even eavesdrop on,
traffic to other sites, nor can it generate “spoofed” network frames
that purport to be from some other site.  Also, it cannot install
malicious software on the victim's computer; if it could, all
browser-based defenses would be useless.

The web attacker can inject text strings into the target site,
provided that they pass state-of-the-art server-side XSS filters such
as \cite{htmlpurifier}.  In general, a cross-site CSS attack will
require the attacker to inject two strings, one on each side of the
secret to be stolen; however, depending on the structure of the target
page, one string may be sufficient.

Finally, the web attacker can entice the victim into visiting its
site; this is easily done either by social engineering, or by
manipulating an advertisement network.  We do not assume that the
victim discloses any sensitive information while on the attacker's
site; merely rendering the attacker's web content is sufficient.

\section{Cross-Origin CSS Attacks} \label{sec:attacks}

In this section, we present cross-origin CSS attacks in detail.
First, we describe aspects of browser behavior that, together,
make these attacks possible.  Second, we lay out the steps of an
attack on a hypothetical website.  Third, we discuss constraints on
practical executions of the attack.  Finally, we demonstrate that the
attack can be carried out against several popular web applications.

\subsection{Browser Behavior} \label{sec:behavior}

Cross-origin CSS attacks are possible because of existing browser
behaviors, reasonable taken in isolation, but with unexpected
interactions: session authentication, cross-origin resources,
error-tolerant style sheet parsing, and content sniffing.

\subsubsection{Session Authentication}
Web applications that handle sensitive data typically use client-side
state to manage a distinct “session” for each visitor.  The most
common technique uses HTTP cookies~\cite{rfc2109,httpstate} to define
a session; HTTP authentication~\cite{rfc2617} is also viable, but less
popular since it gives the application less control over user
experience.  Either way, once a user has logged into a web
application, their browser will transmit a credential with every HTTP
request to that server, allowing the server to identify the session
and reply with HTML documents containing confidential information
intended only for that user.  A request for the same URI without the
credential produces an HTTP error, or a generic document with no
confidential information.

\subsubsection{Cross-Origin Resources}
As discussed in the Introduction, browsers permit web pages to
reference resources (images, scripts, style sheets, etc.)\ from any
origin, not just from the server hosting the page itself.  Requests
for cross-origin resources transmit any credentials (cookies or HTTP
authentication tokens) associated with the site that hosts the
resource, \emph{not} credentials associated with the site whose page
made the reference.  Thus, confidential information from one site can
be “transcluded” into a page that could not read it directly.  The
browser prevents scripts in the requesting page from inspecting any of
the embedded content, so this should be safe.  However, cross-origin
request forgery (CSRF) attacks~\cite{csrf} exploit this in conjunction
with the common use of URIs to name \emph{commands} as well as
resources; given the right HTML, the browser will cheerfully try to
load an image from \url{http://example.com/logout} or
\url{http://example.com/post12345?action=delete}.

\begin{figure*}[t]
\begin{center}
\includegraphics[width=5in]{victim-manipulation}
\vskip 0.5em
\setlength{\tabcolsep}{0.12in}
\begin{tabular}{p{1.5in}p{1.5in}p{1.5in}}
\centering
HTML document; secret data is highlighted.&
\centering
Attacker injects CSS leader and trailer around secret.&
\centering
CSS parser skips most of the document, makes secret
available via computed style.
\end{tabular}
\end{center}
\caption{Anatomy of the attack.}
\label{figure:victim}
\end{figure*}

\subsubsection{Error-Tolerant Style Sheet Parsing} \label{sec:lax}
CSS syntax has much more in common with JavaScript than with HTML.
HTML uses angle brackets to delimit \emph{tags} that must nest; text
outside tags is mostly unparsed. CSS and JavaScript both use curly
braces to enclose \emph{blocks}; inside or outside a block, the input
text must follow a formal grammar.  However, CSS's \emph{keywords} are
almost entirely different from JavaScript's keywords.

When browsers encounter syntax errors in CSS, they discard the current
syntactic construct, skip ahead until what appears to be the beginning
of the next one, then start processing again.  The CSS
specification~\cite{syndata} defines precisely how this must be done,
so that browsers will behave predictably when they see new CSS
features they do not understand.  When skipping ahead, the browser
uses only a few simple grammar rules:

\begin{itemize}
\item Depending on where the syntax error occurred, the next syntactic
  construct might begin after the next semicolon, after going up one
  brace level, or after the next brace-enclosed block.
\item While skipping, parentheses, square brackets, and curly braces
  must still be properly balanced and nested.
\item Unlike in HTML, angle brackets are not expected to balance.
\item \verb|/* ... */| is a comment to be ignored, as in JavaScript.
  However, unlike JavaScript, \verb|//| does \emph{not} indicate the
  beginning of a single-line comment.
\item Single- and double-quoted strings also work as in JavaScript;
  backslash escapes are a little different, but this doesn't matter
  for our purposes.  Internet Explorer permits strings to extend past
  a line break, but in all other browsers this is a syntax error.
\item The end of a style sheet closes all open constructs
  \emph{without error}.
\end{itemize}

The left angle bracket, \texttt{<}, so common in HTML, has no meaning
in CSS; it will invariably cause a syntax error.  (The right angle
bracket, \texttt{>}, can appear within CSS selectors.)  Thus, a CSS
parser encountering an HTML document will go into skip-ahead mode on
the very first tag in the document, and will probably stay there until
the end of the file.

\subsubsection{Content Sniffing} \label{sec:mime}
The HTTP \texttt{Content-Type} header is supposed to indicate the type
of the content being transmitted, using the Multipurpose Internet Mail
Extensions (MIME) type vocabulary~\cite{mime}; for instance, HTML is
labeled \texttt{text/html}, CSS is labeled \texttt{text/css}, and a
JPEG-format photograph is labeled \texttt{image/jpeg}.  For content
that is, at root, textual, \texttt{Content-Type} is also supposed to
indicate the character encoding.

Unfortunately, the \texttt{Content-Type} header is unreliable in
practice; the situation is better than it was ten years ago, but
misconfigured HTTP servers still frequently omit this header, or
produce it but with incorrect information.  Therefore, browsers must
use “content-sniffing” algorithms~\cite{securecontentsniffing} that
inspect the first few bytes of the HTTP response body, as well as the
\texttt{Content-Type} header, to decide how to process resources.  The
security problems this causes have been well-covered elsewhere.

CSS is also frequently delivered with an incorrect content type, usually
\texttt{text/plain} or \texttt{text/html}.  A valid style sheet can
begin with an enormous number of different byte sequences, so browsers
do not attempt to content-sniff for CSS.  Instead, they allow a
document in “quirks mode”~\cite{quirksmode} to load \emph{anything at
  all} as a style sheet, regardless of its \texttt{Content-Type}
header.  (In “standards mode,” style sheets must be delivered with the
correct content type, or they are ignored.)

\begin{table*}
\centering
\footnotesize
\begin{tabular}{crccccc}
\toprule
Approach&\multicolumn{1}{c}{API}&IE&FF&Opera&Safari&Chrome\\
\midrule
CSS Object Model&
  \texttt{styleSheets[].cssRules[].cssText}&&&&\checkmark&\checkmark\\
 &\texttt{getMatchedCSSRules().cssText}&&&&\checkmark&\checkmark\\
\addlinespace
Computed Style&
  \texttt{getComputedStyle}&&\checkmark&\checkmark&\checkmark&\checkmark\\
 &\texttt{currentStyle}&\checkmark&&\checkmark&&\\
\addlinespace
No Javascript&
  \texttt{background-image}, etc.&
  \checkmark&\checkmark&\checkmark&\checkmark&\checkmark\\
\bottomrule
\end{tabular}
\caption{Methods of Extracting Information from Cross-Origin Style Sheets}
\label{table:DOM}
\end{table*}

\subsection{Attack Steps}
In a cross-origin CSS attack, the attacker wishes to steal data that
should only be revealed to a particular user (the \emph{victim}) from
a web site it does not control (the \emph{target}).  The stolen data
may be itself of value (e.g.\ a private e-mail message), or it may
enable another attack (e.g.\ a token embedded in the target document
to block CSRF attacks).  To do so, the attacker first injects strings
into the target document that bracket the data to be stolen; then it
loads the target document as if it were a style sheet for a page under
its own control.  (Since the attacker controls this page, it can
ensure that “quirks mode” is in effect, so the content type
of the target document does not matter.)  The injected strings cause
the CSS parser to ignore most of the document and extract the secret,
bypassing the same-origin policy.  Figure~\ref{figure:victim}
illustrates these steps.

\subsubsection{CSS String Injection}
One might expect that an HTML document, when parsed as a style sheet,
would produce nothing but syntax errors.  However, because of the
predictable error recovery rules described in section~\ref{sec:lax},
it is possible to inject strings into a document that will cause the
CSS parser to come out of error recovery mode at a predictable point,
consume some chunk of the document as a \emph{valid} rule, and then
return to skipping.  Attackers have many options for injecting text
into a web page, even one they cannot see without authentication.  In
\cite{cssxss08} the attacker created an account on the target site
with a carefully crafted user handle, then induced the victim to view
the account profile.  Our demonstration attacks in
section~\ref{sec:demos} use intra-site private messages or junk
email sent to the victim.

In the middle document in figure~\ref{figure:victim}, the attacker has
arranged to insert two strings into the document:
\begin{itemize}
\item \verb|{}#f{font-family:'| before the secret
\item \verb|';}| after the secret
\end{itemize}
The target site has wrapped each of these in an HTML \verb|<span>|,
which is harmless to the attacker's purpose.  The opening string has
three components: The attacker can safely assume that the CSS parser
is in error recovery mode, looking for a brace-enclosed block, when it
encounters the two-character synchronization sequence \verb|{}|.  This
sequence will take the CSS parser out of error recovery, unless there
is something before the injection point that must be balanced---an
unclosed string or CSS comment, or an unmatched \verb|{| \verb|[| or
\verb|(|.  If the attacker can predict what comes before the injection
point, it can tailor the synchronization sequence to match.  The next
component, \verb|#f{font-family:| is the beginning of a valid CSS
style rule, declaring the font family for an element in the attacker's
document (with ID \texttt{f}).  The \texttt{font-family} property
takes a string constant as its value; thus the final component of the
opening string is a single quote character, \verb|'|.  The CSS parser
will absorb whatever follows as a string, as long as it contains
neither line breaks nor another single quote.  (The text in
figure~\ref{figure:victim} has been word-wrapped for readability; if
any of the line breaks in between the injected blocks were actually
present, the attack would only work in IE.)

The closing string simply ends the CSS string constant with another
quote mark, and then closes the style rule with a semicolon and a
close brace.  (The semicolon could be omitted.)  Regardless of what
appears after the close brace, this style rule has been successfully
parsed and will affect the attacker's document.

\subsubsection{Cross-Origin CSS Import}
When the victim user visits \texttt{attacker.com}, the attacker's page
instructs the victim's browser to fetch and load the target document,
with its injected strings, as an external style sheet.  This can be
done with the \texttt{link} tag~\cite{html}:

\verb|<LINK REL="stylesheet" HREF="http://target.com">|

or with the CSS “import” directive, in an internal style sheet:

\verb|<STYLE>@import url(http://target.com);</STYLE>|

The attacker must ensure that their page is in “quirks mode,” but this
is easy to do: simply do not begin the page with any \verb|<!DOCTYPE|
declaration, and do not serve it as XHTML.

\subsubsection{Confidential Data Extraction}\label{sec:extraction}
Having loaded the target document as a style sheet, the attacker must
finally extract the secret from the style rules.  There are three
approaches, some of which are more convenient and some of which work
under more conditions; Table~\ref{table:DOM} summarizes them.
JavaScript-based approaches transmit the stolen data to the attacker's
server using \texttt{XMLHttpRequest} or a hidden form; the
non-JavaScript approach uses a carefully constructed URL instead.

\paragraph{CSS Object Model}
JavaScript can read the text of successfully parsed style rules via
the \texttt{cssText} property of \emph{style rule} objects.  All the
style rules for a document are visible in the
\texttt{document.styleSheets[].cssRules[]} arrays.  Safari and Google
Chrome also provide the \texttt{getMatchedCSSRules} utility function
that can retrieve style rules matched by an element.  This is perhaps
the most convenient way to extract secrets, but it only works in
Safari and Chrome.  IE, Firefox, and Opera have blocked JavaScript
access to style rules from sheets loaded cross-origin since 2002 (in
response to~\cite{cssxss02}).  In the example in
figure~\ref{figure:victim}, \texttt{cssRules[0].cssText} would expose
all of the text that isn't struck out in the right-hand document.

\paragraph{Computed Style}
JavaScript can also inspect the style currently applied to an element,
never mind how it got that way.  This variant of the attack is
slightly less convenient; the attacker must ensure that the style rule
produced by the attack does apply to some element in the attacking
page (not in the target page), and the JavaScript code required is not
fully portable.  Most browsers support the standard function
\texttt{getComputedStyle}, but for IE one must use the
\texttt{currentStyle} object.  However, no current browser blocks
access to computed styles based on origin, so this variant works in
any current browser as long as JavaScript is enabled.
\texttt{getComputedStyle(f).style.fontFamily} would expose the text
highlighted in white in the right-hand document in
figure~\ref{figure:victim}.

\paragraph{Without JavaScript}
This attack is even possible if users have disabled JavaScript,
although not as shown in Figure~\ref{figure:victim}.  Several CSS
properties can direct the browser to load an arbitrary URL; for
instance, the attacker might change their injected strings to

\begin{itemize}
\item \verb|{}#f{background:url('http://attacker.com/?|\\
  before the secret
\item \verb|');}| after the secret
\end{itemize}

As long as there is an element in the attacking page that matches this
rule, the browser will issue a GET request to the attacker's server
and provide the secret to be stolen as the query string.  This
approach is perhaps \emph{more} convenient than the previous ones if
the only goal is to steal data, since the attacking page can be
simpler.  It is somewhat less convenient for bootstrapping a CSRF
attack, since the secret has to be sent back down from the server
before it can be used, but this is minor compared to the other
difficulties of CSRF without JavaScript.  A
clickjacking~\cite{clickjacking} attack is definitely still feasible.

\subsection{Attack Limitations} \label{sec:limits}
At present, we are not aware of anyone deliberately designing their
HTML or their server-side filters to block cross-origin CSS attacks,
but they can still be thwarted by common characteristics of site
structure, or by filters aiming to block JavaScript attacks.

\paragraph{Injection points}
The secret to be stolen is encapsulated within a CSS string constant,
within a property value, within a style rule.  To do this, the
attacker must inject \emph{two} strings into the document containing
the secret: one to begin the rule, and one to end it.  (By contrast,
JavaScript injection attacks usually require injecting only one
string.)  Sites that accumulate user-submitted text (blog comments,
for instance) are relatively more vulnerable to this attack; the
attacker can inject one string, wait a while, and then inject another.
Also, the string that must appear after the secret is very
simple---often just a close quote and a close bracket---and may
already be present in the target page; this was the case
in~\cite{cssxss08}.

\paragraph{Quotes}
CSS string constants can be written with single or double quotes.
Double quotes cannot occur inside a double-quoted string, and single
quotes cannot occur inside a single-quoted string, unless they are
escaped with backslashes.  Thus, if the secret to be stolen contains
single quotes, the attacker must use double quotes in their injected
strings, and vice versa.  If the secret contains both types of quotes,
or the attacker cannot predict which type of quotes it will contain,
the attack will fail.

If the “without Javascript” variant of the attack is in use, and the
attack only needs to work in Internet Explorer, the attacker may
instead use the unquoted form of \texttt{url()}.  This cannot contain
unescaped parentheses, but in IE it may contain unescaped quotes.

\paragraph{Newlines}
CSS string constants and unquoted \texttt{url()}s cannot contain
newlines, unless they are escaped with backslashes.  Therefore, any
newline within the secret will cause the attack to fail.  HTML pages
tend to contain many newlines; this, all by itself, protects many
potential target sites from CSS data theft attacks.  However,
rich-functionality sites often offer URL-based APIs that deliver
confidential information in a custom JSON or XML format, with no
newlines; these APIs may be vulnerable to CSS data theft even if the
human-visible site isn't.  Some sites even allow their users to
control the formatting of HTML server responses, e.g.\ to disable
pretty-printing; the attacker may be able to do the same.

Internet Explorer permits unescaped newlines in CSS string constants
and unquoted \texttt{url()}s.  This makes the attack dramatically
easier to carry out if the victim uses IE.

\paragraph{Server-side filtering}
Server-side filters aiming to remove malicious code from
user-submitted content are common, but they are usually designed to
strip dangerous HTML attributes and defang JavaScript keywords.  They
will not block cross-origin CSS attacks, because the injected strings
won't be nested within HTML attributes, and CSS shares very few
keywords with JavaScript.

Some filters also replace particular punctuation characters with
equivalent HTML entities.  Single and double quotes are often
replaced, because of their significance in both HTML and JavaScript.
If \emph{any} of the punctuation in the injected strings is replaced
with an entity, the attack will fail.

The attacker may be able to defeat character replacement by
pre-encoding the replaced characters in UTF-7~\cite{utf7}.  For
instance, if the target site replaces single quotes with entities, but
leaves the other punctuation alone, the injected strings would become
\begin{itemize}
\item \verb|{}#f{font-family:+ACc-| before the secret
\item \verb|+ACI-;}| after the secret
\end{itemize}
The attacker would then request UTF-7 decoding from the CSS parser,
by specifying a character set in their \verb|link| tag:

\verb|<LINK REL="stylesheet" HREF="http://target.com"|\\
\verb| CHARSET="utf-7">|

This trick does not work if the target site specifies a character set
in its \texttt{Content-Type} header.  Unfortunately, only 584 out of
the top 1,000 web sites ranked by Alexa~\cite{alexa} specify character
sets for their home pages in their \texttt{Content-Type} headers.
Many of the others do provide character set information in a
\verb|meta| tag, but the CSS parser pays no attention to HTML
\verb|meta| tags, so that will not thwart an attacker's specification
of UTF-7 in a \verb|link| tag.  (Here we see another reason sites
should always provide correct \texttt{Content-Type} headers.)

\subsection{Example Attacks} \label{sec:demos}
We have successfully carried out cross-origin CSS attacks on several
popular websites.

\paragraph{IMDb}
IMDb is an online database of movies and related information, which
allows registered users to rate films, make posts on message boards,
and send private messages to each other.  An attacker with an account
on the site can steal the text of private messages to a victim user,
with these steps:

\begin{enumerate}
\item Send a private message to the victim's account, with the subject
  line: \verb|{}body{font-family:'|
\item Induce the victim to visit \texttt{attacker.com} while signed
  into IMDb; the attacking page is as follows:
\begin{verbatim}
<html>
<head>
<link rel="stylesheet"
     href="http://www.imdb.com/user/
           ur12345678/boards/pm/">
<script>
function steal() {
  alert(document.body.
    currentStyle["fontFamily"]);
}
</script>
</head>
<body onload="steal()">
</body>
</html>
\end{verbatim}
\end{enumerate}

The attacker must know the victim's account ID (\texttt{ur12345678} in
the example); this is public information that can be learned from the
victim's user profile page, even if one is not logged in.  The browser
will retrieve the victim's private messaging page, using the
appropriate credentials from the victim's IMDb session, and process it
as a style sheet.  The private message sent in step 1 will cause a
fragment of HTML, including the full text of earlier private messages
to the victim, to be absorbed as a CSS property value, which is then
revealed to JavaScript via \texttt{currentStyle}.

This attack works only in IE, due to newlines in the HTML for the
private messaging page.  This is why the JavaScript above uses only
the IE-specific mechanism for retrieving the computed style.  It is
not necessary to inject a second string after the text to be stolen,
because the end of the page serves that purpose (recall that end of
style sheet closes open CSS constructs without error).

\paragraph{Yahoo! Mail}
Yahoo! Mail is a popular web-based email service.  Its session cookies
persist for up to two weeks if users do not actively log out.  An
attacker can steal subject lines and CSRF tokens from a victim's email
inbox with these steps:

\begin{enumerate}
\item Send an email to the victim with the subject line:
  \verb|');}|
\item Wait for some time while the victim receives other messages.
\item Send another email to the victim with the subject line:
  \verb|{}body{background-image:url('|
\item Induce the victim to visit \texttt{attacker.com} while signed
  into Yahoo! Mail.  The attacking page is as follows:
\begin{verbatim}
<html>
<head>
<link rel="stylesheet"
     href="http://m.yahoo.com/mail">
<script>
function steal() {
  if(document.body.currentStyle) {
    alert(document.body.
      currentStyle["backgroundImage"]);
  } else {
    alert(getComputedStyle(document.body, "").
      backgroundImage);
  }
}
</script>
</head>
<body onload="steal()">
</body>
</html>
\end{verbatim}
\end{enumerate}

We use \texttt{background-image} instead of \texttt{font-family} in
this attack to illustrate the variety of CSS properties that can be
used.  The attacking page requests the mobile version of the site by
loading \url{http://m.yahoo.com/mail} rather than
\url{http://www.yahoo.com/mail}.  To reduce bandwidth requirements,
the mobile site has all unnecessary whitespace removed from its HTML,
including newlines; this allows the CSS portion of the attack to
succeed in more browsers, hence the JavaScript must detect which of
the two methods for retrieving computed style is supported.

The stolen HTML fragment contains the subject lines of every email
delivered to the victim in between the two attack messages.  It also
contains a hidden, unguessable token for each message; knowing these
tokens allows the attacker to delete messages via CSRF.

\paragraph{Hotmail}
Windows Live Hotmail is another web-based email service, operated by
Microsoft rather than Yahoo!.  It is vulnerable to nearly the same
attack as Yahoo! Mail: we can read messages and acquire CSRF tokens by
sending two emails to a victim Hotmail account with crafted subject
lines, then loading the mobile Hotmail website as a style sheet.
Unlike the mobile version of Yahoo! Mail, Hotmail delivers HTML
containing newlines, which limits the attack to Internet Explorer.

The existence of nearly identical attacks on unrelated websites
illustrates the general nature of cross-origin CSS vulnerabilities. We
expect that many social networking sites are vulnerable to variants of
this attack as well, because the attacker can leave arbitrary text
comments that are rendered somewhere on the victim's view of the page.

\section{Defenses} \label{sec:defenses}

In this section, we propose a client-side defense against cross-origin
CSS attacks, evaluate it for compatibility with existing web sites,
and review its adoption by major browsers.  We also examine a few
alternative client-side defenses and complementary server-side measures.

\subsection{Proposal: Restrict Cross-Origin Loads}
   \label{sec:proposal}

\begin{table*}
\centering
\def\m#1#2{\multicolumn{#1}{c}{#2}}
\begin{tabular}{crrrrrrr}
\toprule
  Requesting&      \m1{Page}&        &           &      \m2{Correct type}&    \m2{Incorrect type}\\
      server& \m1{rendering}&   Total& HTTP error& Well-formed& Malformed& Well-formed& Malformed\\
\midrule
            &          Total& 260,069&      2,363&     255,698&       999&         949&        60\\
\addlinespace
 Same-Origin& Standards Mode& 180,445&      1,497&     178,017&       506&         424&         1\\
            &    Quirks Mode&  25,606&        466&      24,445&       332&         304&        59\\
\addlinespace
Cross-Origin& Standards Mode&  47,943&        347&      47,345&       104&         147&         0\\
            &    Quirks Mode&   6,075&         53&       5,891&        57&          74&         0\\
\bottomrule
\end{tabular}
\caption{Categorization of CSS references for Alexa 100,000 sites.}
\label{table:results}
\end{table*}

Cross-origin CSS attacks only work because the browser will attempt to
parse \emph{anything} that was requested by a stylesheet \verb|link|
or \verb|@import| as if it were CSS.  This is a backward compatibility
feature, part of the “quirks mode” applied to HTML documents that do
not include a proper document type definition (DTD).  In the
“standards mode” recommended for new content, style sheets will only
be processed if they are labeled with the HTTP header
\verb|Content-Type: text/css|.

The attacker, of course, controls whether or not the attacking page is
in quirks mode.  However, the attacker has no control over the
\texttt{Content-Type} header labeling the \emph{target} page; that's
generated by the target site's server.  Therefore, our proposed
client-side defense is to enforce content type checking for
style sheets loaded cross-origin, even if the requesting page is in
quirks mode.  The target page is going to be labeled \verb|text/html|,
or similar (\verb|application/json|, \verb|text/rss+xml|, etc.)\ so
the browser will not load it as a style sheet and the attack will be
foiled.

\subsubsection{Strict or Tolerant Enforcement} \label{sec:enfmodes}
If we refuse to load \emph{any} style sheet cross-origin unless it is
properly labeled with \verb|Content-Type: text/css|, legitimate
requests for cross-origin style sheets may fail, when the server
providing the style sheet is misconfigured.  As we mentioned above,
content type misconfigurations are common, so this strictly
enforced rule may be too risky for browser vendors to adopt.

To address this concern, we also propose a more tolerant enforcement
mode: when the browser encounters a cross-origin style sheet labeled
with the wrong content type, it begins parsing the sheet as
CSS, but if it encounters a syntax error before it has processed the
first complete style rule, it stops and discards the sheet.  This rule
allows legitimate but misconfigured sites to continue to work, as long
as the first thing in their cross-origin, mislabeled style sheet is a
well-formed CSS rule.  But it still blocks most cross-origin CSS
attacks, which attempt to load an HTML document as CSS; it is almost
certain that the first thing in the file will be \verb|<html>| or
\verb|<!DOCTYPE|, either of which is a syntax error to CSS.

\subsection{Experiment}
To inform a choice between strict and tolerant enforcement, we
surveyed the public Web to determine how often servers fail to provide
the correct content type for style sheets, how often style
sheets begin with a CSS syntax error, and how often style sheets are
requested from a different origin.

\begin{table}[b]
\centering
\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{Incorrect \texttt{Content-Type}}&
\multicolumn{2}{c}{Occurrences}\\
\midrule
               \texttt{text/html}& 715& (71\%)\\
              \texttt{text/plain}&  45&  (4\%)\\
\texttt{application/octet-stream}&  29&  (3\%)\\
                            other&  42&  (4\%)\\
                           absent& 178& (18\%)\\
\bottomrule
\end{tabular}
\caption{Incorrect content types observed for CSS.}
\label{table:MIME}
\end{table}

\paragraph{Design}
Using an instrumented browser based on WebKit~\cite{webkit}, we
crawled the top 100,000 web sites ranked by Alexa~\cite{alexa} and
identified all of the style sheet resources used by their front pages.
Our instrumentation reported every style sheet requested while the
page itself was loading.  This allowed us to identify sheets used
indirectly via CSS \verb|@import| directives, and sheets added by
JavaScript during page load, as well as those mentioned directly in
the HTML.

% This table is here rather than with “Adoption” so it'll come out in
% the right place in the printed document.
\begin{table*}
\centering
\begin{tabular}{lccccccc}
\toprule
\multicolumn{1}{c}{\texttt{Content-Type}}
&Opera&Safari&Chrome&Firefox 3.5/3.6&Firefox 4&IE 7/8&IE 9\\
\midrule
\texttt{text/html},
other well-formed non-CSS                   & T & T & T & T & S & ? & ?\\
\texttt{*/*}, other ill-formed values       & T & T & T &   &   & ? & ?\\
Header absent                               & T &   &   &   &   & ? & ?\\
\texttt{application/x-unknown-content-type} & T &   &   &   &   & ? & ?\\
\bottomrule
\multicolumn{8}{r}{\vrule height10pt width0pt\relax\itshape
  S = strict defense; T = tolerant defense; blank = no defense.}
\end{tabular}
\caption{The effect of missing or ill-formed
  Content-Type}\label{table:adoption}
\end{table*}

\paragraph{Results}
From these 100,000 web sites, our crawler logged a total of 260,069
CSS references, of which 206,051 were same-origin and 54,018
cross-origin.  We did not include data for sites that were unreachable
during our evaluation, due to unresponding servers or domain name
errors. Our results are shown in Table~\ref{table:results}.

Of these 260,069 requested style sheets, 2,363 returned an HTTP error
(e.g.\ 400 Bad Request, 404 Not Found, or 500 Internal Server Error)
rather than a style sheet.  All current browsers discard a style sheet
when they receive an HTTP error, so we did not analyze the body or
\texttt{Content-Type} header for these responses.

Of the remaining 257,706 sheets, only 1,009 were labeled with an
incorrect \texttt{Content-Type} header (that is, anything but
\verb|Content-Type: text/css|.  We summarize the incorrect headers we
observed in Table~\ref{table:MIME}; \verb|text/html| is the most
common value, accounting for 71\% of errors.  Some of these
\verb|text/html| responses were HTML landing pages produced (with a
200~OK response code) because the desired style sheet no longer
existed; the content type is correct in this case, but the
server is still misconfigured, as it should have produced an HTTP
error.  Style sheets labeled with the generic types \verb|text/plain|
and \verb|application/octet-stream| make up a further 7\% of the
total, and a few other specific types appeared,
e.g.~\verb|application/x-javascript|.

The second most common error, accounting for 18\% of the total, is to
provide no \texttt{Content-Type} header at all, or a header with no
value; these are listed together in table~\ref{table:MIME} as
“absent.”  Most browsers will process a style sheet with an absent
content type, even in standards mode.  See
section~\ref{sec:missing} for further discussion of this wrinkle.

The crawler logged whether standards or quirks mode was in effect for
each HTML page that loaded a CSS resource.  Quirks mode is in effect
for a substantial minority of the 100,000 sites crawled, but of the
260,069 requests for CSS, only 31,681 came from sites in quirks mode.
In standards mode, style sheets are always discarded if they are
labeled with the wrong content type; we observed 572 such
futile requests in our sample.  From sites in quirks mode, there were
437 requests for sheets that were labeled with the wrong type; these
sheets are honored.

The crawler also recorded whether a style sheet was served from the
same origin as the requesting HTML document.  It is most common to
serve style sheets from the same origin as the HTML, but we did
observe 54,018 cross-origin requests, 6,075 of which were for pages in
quirks mode.  Only 74 of those cross-origin requests were labeled with
the wrong content type.

Finally, the crawler checked whether each sheet began with a
well-formed CSS construct.  1,059 sheets (0.41\% of the sample) were
not well-formed.  (It is interesting to note that a common error among
these ill-formed sheets is to start the file with an HTML
\verb|<style>| tag.)  Only 60 sheets were both malformed and labeled
with an incorrect content type, and none of these were served
cross-origin.

\paragraph{Discussion}
Within the Alexa top 100,000 web sites, we observed a total of 1,009
CSS resources labeled with an incorrect content type (excluding responses
with HTTP errors).  Of these, 572 are associated with sites being
rendered in standards mode, and are therefore already being ignored.
Of the remaining 437 style sheets, 74 are loaded cross-origin; these
are the sheets that would be rejected by the strict defense, breaking
62 (0.06\%) of the Alexa sites.  This is enough to make browser
vendors reluctant to deploy strict enforcement.  The tolerant mode,
which accepts cross-origin, mislabeled sheets unless they are also
malformed, would not break any of the Alexa sites.

Many of the Alexa top 100,000 sites provide additional content to
registered users.  We did not attempt to create accounts on any of the
sites; our results are strictly for unauthenticated access.  It is
possible that more sites would be broken (by either form of the
defense) if viewed by an authenticated user.

\subsection{Adoption}
Our proposal has already been adopted by several major browsers.  We
implemented tolerant enforcement for WebKit, and both tolerant and
strict enforcement for Mozilla's Gecko engine.  Tolerant enforcement
based on our changes has been deployed in Google Chrome
(version~4.0.249.78), Safari (version~4.0.5), and both Firefox~3.5.11
and~3.6.7.  Firefox~4 instead offers strict enforcement, which Mozilla
considers preferable in the long term.  Opera independently
implemented tolerant enforcement for version~10.10 of their browser.

\subsection{Missing or Ill-Formed Content Types}\label{sec:missing}
To be fully reliable, our proposed defenses should be applied whenever
a style sheet lacks the \verb|Content-Type: text/css| label, including
when the \texttt{Content-Type} header is missing or has an ill-formed
value.  Recall from Table~\ref{table:MIME} that we saw 178 CSS
resources that lacked a Content-Type header in our survey.  However,
as shown in Table~\ref{table:adoption}, most browsers---with the
notable exception of Opera---do accept cross-origin style sheets if
they lack a \texttt{Content-Type} header, even in standards mode.
Firefox ignores \texttt{Content-Type} headers that it cannot parse
(e.g.~\verb|Content-Type: */*|) and will therefore also accept a
cross-origin style sheet with an unparseable \texttt{Content-Type}.
Finally, Webkit and Firefox both treat the special type
\texttt{application/x-unknown-content-type} the same as the absence of
a header.

These gaps in the defense could open up a target server to attack, if
it fails to set a \texttt{Content-Type} header on its HTML
documents. We have not yet observed any web servers in the wild that
are affected by this vulnerability, but browsers may wish to follow
Opera's lead and block such style sheets when loaded across
origins. In any case, we recommend that servers always provide a
correct Content-Type header.

\subsection{Other Client-Side Approaches}
Other defensive approaches could be deployed in browsers without
modifying web servers, but we claim that all of them could easily be
circumvented, or else would significantly reduce web compatibility.

\paragraph{Block Cookies}
If HTTP cookies are disabled in the browser, web attackers cannot
steal content from cookie-au\-then\-ti\-cated sites.  However, completely
disabling cookies renders many sites unusable.  Some browsers have the
option to block only “third-party” cookies, which prevents cookies
from being \emph{set} by a cross-origin load.  Unfortunately, this
mode typically does not block cookies from being \emph{sent} with a
cross-origin load, because some sites require session cookies for
cross-origin resources~\cite{jackson06thirdpartycookies}.  Blocking
only cookie sets does not block cross-origin CSS attacks.

\paragraph{Block JavaScript Style APIs}
Many browsers already prevent JavaScript from reading parsed style
rules when those rules were loaded cross-origin; this could be done
more thoroughly, and they could also prevent access to computed style
when the chosen value came from a cross-origin sheet.  These changes
would stop some attacks, but an attacker could still use the
\texttt{background-image} approach that does not require JavaScript.

\paragraph{Stricter CSS Parsing}
Cross-origin CSS attacks would be much more difficult if the CSS
parser threw away the entire style sheet on any syntax error.
Unfortunately, this would ruin CSS's forward compatibility feature,
and would break many existing web sites.

\subsection{Server-side Mitigation}
Web sites have several options for rendering themselves less
vulnerable to cross-origin CSS attacks.  Despite the rapid uptake of
our proposed client-side defense, we still recommend that site
maintainers consider these server-side mitigations, to protect site
users who are still using a browser with no defense.

\paragraph{Newlines}
The CSS specification does not allow strings and URLs to contain
newlines.  Most browsers honor this rule, so sites can defend against
cross-origin CSS attacks by inserting newlines before and after
potential injection points.  However, this does not protect users of
Internet Explorer, which at the time of writing is the only major
browser not to have adopted a defense.

\paragraph{HTML encoding}
As we mentioned in section~\ref{sec:limits}, CSS-based attacks can be
prevented by replacing the punctuation within the injected strings
with HTML entities.  Existing filters often already do this for
quotation marks, but quotation marks are not required for the attack;
the attacker could use an unquoted \texttt{url()} instead.  We
recommend escaping curly braces in user-submitted content as well,
using \verb|&#123;| and \verb|&#125;|.  This will block all known
forms of the attack, as long as the attacker cannot force UTF-7
encoding (see below).  Unfortunately, at present, the utility
routines provided by popular scripting languages will not
entity-encode curly braces.

\paragraph{Content-Type}
Our browser-side defense is ineffective for target sites that do not
label all HTTP responses, especially those that contain confidential
information, with the proper \texttt{Content-Type} header, as
discussed in section~\ref{sec:missing}.  It is also important to
ensure that the \texttt{Content-Type} header includes a character set
declaration.  Otherwise, the attacker may be able to defeat HTML
entity encoding of quotes and curly braces by forcing the target page
to be interpreted as UTF-7.  Declaring the character set in a
\texttt{meta} tag inside the document is not good enough, because the
CSS parser will not recognize that tag.

If it is impossible to get a character set declaration into the HTTP
headers for some reason, entity-encoding plus signs as \verb|&#43;|
would also prevent the attacker from using UTF-7.

\paragraph{Avoid Ambient Authentication}
Cross-site attacks rely on the browser transmitting “ambient”
authentication information, such as HTTP credentials or session
cookies, with any request to the target site.  A site that makes no
use of these is less vulnerable.  One possible alternative is the
web-key authentication scheme~\cite{webkey}, which embeds credentials
in site URLs instead of cookies.  The attacker is foiled, as they
cannot guess these URLs.  However, if a URL with a credential becomes
visible to the victim user (e.g.\ via the location bar), they might be
tricked into revealing it; sites must assess whether this is an
acceptable trade-off.

\section{Related Work} \label{sec:relatedwork}
In this section, we review current client-side defenses against
similar attacks: content-sniffing XSS and JavaScript hijacking.  We
also look at a few recent research proposals for secure web browsers
in the light of the cross-origin CSS attack.

\subsection{Content-Sniffing XSS}
Browsers use content-sniffing algorithms to detect HTML documents that
were not properly labeled by the server, as discussed in
section~\ref{sec:mime}.  Web sites that allow their users to upload
files also use content-sniffing, to ensure that only files in benign
formats (e.g.\ images) are accepted.  When the site's sniffing
algorithm is not the same as the browser's, an attacker may be able to
construct a “chameleon” document that a website believes is benign,
but that a browser will recognize as
HTML~\cite{securecontentsniffing}.  For example, a file beginning with
\verb|GIF<HTML| will be treated as an image by some versions of
MediaWiki, but as HTML by some versions of Internet Explorer.

To deal with this attack,~\cite{securecontentsniffing} proposes a
single, trusted sniffing algorithm that can be adopted universally.
The signatures it looks for are \emph{prefix-disjoint}, which excludes
the possibility of chameleon documents.  It also pays attention to the
\texttt{Content-Type} header and will not \emph{escalate} a document's
capabilities---for instance, it will never treat a \verb|text/plain|
document as HTML, because HTML can contain scripts and plain text
can't.  Microsoft proposed an alternative solution, a new HTTP header
\verb|X-Content-Type-Options| to allow sites to opt out of content
sniffing~\cite{nosniff}.  This is technically simpler, but much harder
to deploy globally.

Both of these solutions aim to ensure that if the server believes a
particular document not to be HTML, the browser will not process it as
HTML.  They do nothing against the cross-origin CSS attack, which
tricks the browser into processing an HTML document as CSS; as
mentioned in section~\ref{sec:mime}, there is no content sniffing for
CSS.

\subsection{JavaScript Hijacking}
Subsets of JavaScript syntax are commonly used as a data transport
format; the most popular of these is JavaScript Object Notation
(JSON)~\cite{json}.  Since the browser security model allows importing
scripts from a different domain, an attacker can steal data in this
format by mentioning its URL in a \texttt{script}
tag~\cite{jshijacking}; as with a cross-domain CSS load, this sends
HTTP credentials for the target site.  Servers can block this attack
by prefixing their JSON responses with a JavaScript statement that
causes a syntax error or infinite loop.  The legitimate client
application for which the response is intended, is coded to strip this
prefix before parsing the JSON, but the malicious page's
\texttt{script} tag evaluates the entire response and will not get
past the prefix.  Servers may also be able to mitigate the attack by
using JSON responses only for HTTP POST requests; the \texttt{script}
tag always generates GET requests.  However, this may require
significant redesign of the web application.  Finally, avoiding
ambient authentication as in \cite{webkey} is also an effective
defense for this attack. \todo{Is Content-Type enforced for
  script loads?  Would serving JSON as application/json be
  useful?}

\subsection{OP Browser}
The OP web browser~\cite{op-browser} uses sandboxing techniques to
isolate and contain failures in browser components.  OP's architecture
does not provide any automatic protection against cross-origin CSS
attacks, which depend only on the high-level behaviors described in
section~\ref{sec:behavior}.  However, OP does maintain a detailed
security audit log that could be used by forensics experts to identify
the site where the attack originated.

\subsection{Gazelle Browser}
The Gazelle browser~\cite{gazelle} includes strict architectural
control over resource protection and sharing across websites.  In
their architecture, sites are treated as security principals, and all
cross-principal communication is explicitly mediated by the browser
kernel to prevent cross-origin attacks.  Cross-origin resources are
only retrieved if the content has the proper content type in the HTTP
response; thus Gazelle implements what we described in
section~\ref{sec:enfmodes} as “strict enforcement” of cross-origin CSS
labeling, as a natural consequence of their security architecture.
Users of Gazelle are protected against cross-origin CSS attacks, at
some cost in site incompatibility (62 out of 100,000 sites in our
survey).

\subsection{SOMA}
The Same Origin Mutual Approval (SOMA) proposal~\cite{soma} restricts
communication between origins by requiring mutual approval between a
web page's server and the servers of its cross-origin resources.  Each
server provides two well-known URLs declaring its cross-origin policy.
One lists all sites \emph{to} which its operators expect to make
cross-origin requests, and the other dynamically reveals whether a
cross-origin request \emph{from} another site is acceptable.  Browsers
are modified to check both policy URLs before making any cross-origin
request.  This design prevents leaking confidential data to unapproved
sites, and so mitigates the cross-origin CSS attack. However, the
negotiation scheme costs additional round-trip requests and requires
modifications to all participating web sites and browsers.

\subsection{CORS}
The Cross-Origin Resource Sharing (CORS) proposal~\cite{cors} is
similar to SOMA, but it uses HTTP headers rather than well-known URLs,
and is strictly for \emph{expanding} the set of sites allowed to
retrieve a resource that would normally be same-origin only.
Initially designed to allow sites to cooperate with
\texttt{XMLHttpRequest}, browser vendors are also considering it for
video, downloadable fonts, and other novel resource types.  These can
be restricted to same-origin by default, and then opened up to
cross-origin requests only when this does not reveal confidential
information.  Thus, CORS reduces the risk of future cross-origin
attacks using novel resource types.  Unfortunately, applying it to
“traditional” resource types such as CSS or JavaScript would break too
many websites to be feasible.  Since it uses HTTP headers, it does not
impose additional round-trip costs, but it may be harder to deploy
than an approach relying on well-known URLs.

\section{Conclusions} \label{sec:conclusion}
In this paper, we presented cross-origin CSS attacks, known for some
time but underestimated as a threat.  We developed a reliable defense
for this attack, in two variants: one strict, based solely on content
types, and one more tolerant, using a content-sniffing rule to improve
site compatibility.  We surveyed 100,000 web sites to assess the site
compatibility of our proposals.  Common server misconfigurations
trigger false positives in the strict variant, and would break
62~(0.06\%) of the 100,000 sites; the tolerant variant does not break
any sites.  Our defense has been adopted in major browsers, including
Firefox, Google Chrome, Safari and Opera.  We also made
recommendations for server-side mitigation of the attack: configure
the \texttt{Content-Type} header correctly, and consider
entity-encoding curly braces in user-submitted content.

\section*{Acknowledgements}

We thank Dave Hyatt, Sam Weinig, Maciej Stachowiak, and Adam Barth of
the WebKit project, and David Baron and Boris Zbarsky of Mozilla, for
reviewing our implementations of cross-origin CSS defenses.

\bibliographystyle{abbrv}
\bibliography{css}

\end{document}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{steps}
\caption{Steps of the Cross-Origin CSS Attack}
\label{figure:steps}
\end{figure*}

