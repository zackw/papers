\documentclass{cmuecequal}
\usepackage[compact,small]{titlesec}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage[font=footnotesize]{caption}
\hyphenation{dis-trib-uted}
\begin{document}

\title{Website Fingerprinting Attacks on Low-Latency Mix Networks}
\author{Zachary Weinberg}
\maketitle

\section{Traffic Analysis and Mix Networks}

Encryption conceals the contents of a message, but an eavesdropper can
still learn the message's length, the time it was sent, where it came
from, and where it went.  \emph{Traffic analysis} seeks to deduce
something about the contents of an encrypted message from these
observable properties.  The source and destination are probably the
most revealing, so more attention has been devoted to concealing them.
One standard technique is a “mix network,” first proposed in 1981 by
Chaum, in which encrypted traffic is relayed through a chain of proxy
servers.  An eavesdropper at any single point in the network may be
able to learn the source of a message, or its destination, but not
both.  Chaining several proxies prevents a malicious proxy from
learning more than a passive eavesdropper could.

Chaum's design was geared to email, and included delays on the order
of hours at each relay, in order to conceal the timing of messages.
This would not be acceptable for synchronous interaction with a remote
server, as is required for Web surfing, voice-over-IP, and so on.  A
\emph{low-latency} mix network does not delay messages, so that it can
be used for these applications, and thus does not conceal message
timing.  For similar reasons, these networks make only limited efforts
to conceal the size of each message.

One of the most popular low-latency mix networks is the “Tor” system,
which is maintained by the loose-knit online organization known as the
Tor Project.\footnote{\url{https://www.torproject.org/}} Its relays
are run by volunteers worldwide.  Due to its open development and ease
of experimentation, Tor is also the most thoroughly studied mix
network in the academic literature.  Much of the research discussed
below uses Tor as a testbed.

\section{Website Fingerprinting}

This paper focuses on “website fingerprinting,” one of the known
classes of traffic analysis attacks on low-latency mix networks.  In a
fingerprinting attack, the adversary attempts to deduce the identity
of a website being visited by a user of the mix network, by observing
the size and timing of messages to and from the target's computer, and
applying a machine classifier to the pattern.

HTTP uses many short-lived, concurrent TCP connections, and the
pattern of connection lifetimes is directly related to the content of
the page being loaded.  However, all of the research we will discuss
studied mix networks that multiplex all traffic into a few long-lived
TCP connections, so the adversary cannot observe this pattern
directly.  Some networks studied, such as Tor, also pad all their
packets to the nearest multiple of 512~bytes, weakly obscuring message
length.

Liberatore and Levine~\cite{Liberatore:2006} is a particularly
influential study of website fingerprinting. They sought to minimize
the amount of information recorded in each fingerprint, and to define
fingerprints that are stable over time, thus not requiring constant
retraining of the classifier.  Their study used a simple proxy server
(OpenSSH in SOCKS tunnel mode) set up for the experiment.

Liberatore and Levine selected sites for their study from a one-month
activity log of all HTTP traffic emanating from their academic
department.  They discarded intra-department servers, PlanetLab
nodes,\footnote{PlanetLab (\url{http://www.planet-lab.org/}) is a
  research testbed, so they assumed it would generate unusual
  traffic.} and traffic generated by automatic processes. This left a
list of 110,000 sites.  The study used the most frequently accessed
2000 of these, which collectively accounted for 64\% of all the
traffic in the log.

They loaded the front page of each of these sites at six-hour
intervals over two months, while recording packet headers.  They then
developed two machine classifiers, one using naive Bayesian statistics
and another that computes the Jaccard coefficient (a similarity
metric) between a challenge and each member of the training set.
These classifiers operated on distilled “traces.”  For the Bayesian
classifier, a trace was an unordered set of counts of packets of each
possible size, with upstream and downstream packets counted
separately, over the course of a page load.  For the Jaccard
classifier, the counts were reduced to Booleans, each size observed or
not.

Except for one graph, their paper considered each classifier to be
successful only if it correctly identified any given challenge; a
“near miss” (misidentification as a statistically similar page) did
not qualify.  The Jaccard classifier was generally more successful
than naive Bayes, but both were credibly effective, correctly
identifying 60 to 80\% of challenge pages, depending on conditions.
These success rates were stable over a period of weeks, and
deteriorated slowly as the set of possible candidates increased.

Liberatore and Levine suggested four possible per-packet padding
schemes as countermeasures: in increasing order of overhead, pad to
nearest multiple of 50 bytes, pad to nearest power of two, pad to
either 100 bytes or the MTU (maximum transmission unit, typically 1500
bytes), and unconditionally pad to MTU.  The Jaccard classifier was
easier to defeat by padding than the simpler Bayes classifier, which
retained a true-positive rate of 0.2 or better against all but
pad-to-MTU.

Since the Liberatore and Levine paper (in 2006) there have been
several other studies of this attack, using various different
classification techniques, training sets, etc.  The recent paper by
Cai et~al.~\cite{Cai:2012} has two interesting innovations.  First,
they used the Damerau-Levenshtein “edit” distance to quantify the
difference between two ordered sequences of packet lengths.  This
metric is defined as the number of insertions, deletions,
substitutions, and transpositions required to convert one ordered
sequence into another.  Cai et~al.~claim that this metric minimizes
the effect of network noise events such as packet loss,
retransmission, and arrival out of order, and is robust against slight
changes in page content.  Having mapped all of the sequences in their
training set into this metric space, they train standard support
vector machine (SVM) classifiers for each site of interest.

Cai et~al.\ trained and tested their classifiers against the top 100
unique, successfully loadable front pages gleaned from the Alexa top
1000 websites.  Each was loaded 20 or 40 times over four different
circumvention systems: a basic SSH tunnel as used by Liberatore and
Levine; SSH plus the HTTPOS ensemble of ad-hoc anti-fingerprinting
tactics (TCP-level randomization of packet sizes, HTTP-level
randomized reordering and division of requests, etc); Tor in its
default configuration; and Tor plus a browser modified to use
“randomized pipelining” at the HTTP level (similar to, but not the
same as, the HTTP-level randomization in HTTPOS).  They derived four
additional hypothetical situations from these by reducing the amount
of information available to the attacker.  In addition to their own
classifiers, they tested two from older papers (Panchenko's ad-hoc SVM
and Herrmann's naive Bayes classifier) in the same environment, for
comparison.

They claim a superior successful-identification rate for their attack,
in all conditions except the hypothetical situation where the attacker
can only observe the total number of packets transferred (in which
case the older ad-hoc SVM does slightly better).  They also claim that
their attack degrades more slowly than the other two as the set of
possible web pages grows; they tested this at intervals from 50 pages
up to 800, still taken from the Alexa top 1000.

Cai et~al.\ also proposed a way to extend fingerprinting to analyze a
sequence of interactions with an entire site, using a hidden Markov
model (HMM) to predict what page loads are likely given the most
recent one.  Separate HMMs for each site analyze a sequence of
probabilistic identifications emitted by the per-page classifier, and
emit the log-likelihood that this is a sequence of interactions with
that model's site.  This allows the adversary to build up confidence
in their classification over a longer period than one page load.  This
technique was not tested as thoroughly as the per-page classifier,
but they did build HMMs for two commonly-blocked sites (Facebook and
IMDB) and try them against actually-observed sequences of page loads,
with somewhat more success for Facebook than for IMDB.

\section{Censorship and Fingerprinting}

A \emph{censor} is an adversary whose primary goal is to block access
to specific content they consider objectionable.  This adversary
normally eavesdrops on a network hop close to the clients that might
try to retrieve objectionable content.  They have two primary
strategies: blocking entire sites known to carry objectionable content
(by hostname or IP address), and scanning for objectionable keywords
within traffic to other sites.  Mix networks defeat both tactics, by
encrypting all traffic and concealing its ultimate destination.  Tor
in particular is commonly used to evade censors.  However, it is
fairly easy for a censor to detect that Tor is in use.  There is a
public directory service that identifies most Tor relays by IP
address, and the wire protocol has a number of distinctive features,
such as an unusual distribution of packet sizes.  Censors can and have
attempted to block all use of Tor.  Our own recent
work~\cite{ccs2012-stegotorus} focuses on concealing the Tor wire
protocol within innocuous “cover” protocols; a related project seeks
to make blocking by IP address impractical.

If a censor cannot prevent access to the Tor network altogether, they
might plausibly attempt to fingerprint websites accessed over it
instead.  If they could identify access to specific objectionable
sites despite the mix network, they could selectively disrupt sessions
accessing those sites.  This strategy, if it worked, would be harder
to evade and less likely to draw negative attention via collateral
damage.  However, it is an open question whether fingerprinting can be
made to work outside the lab.  There are two potential roadblocks:
resource limits on the algorithm, and intractable error rates due to
the sheer size of the global Internet.

\subsection{Resource Limits}

A country-scale Internet censor has unusually stringent time and space
constraints on its operations.  A typical backbone router can spend
only one or two microseconds processing each packet.\footnote{Estimate
  based on the CAIDA Anonymized Internet Traces 2011, in which one
  interface on a backbone router fielded an average of 540,000 packets
  per second, so $1.8\,\mu\hbox{s}$ per packet.} The precise
capabilities of “deep packet inspection” hardware are not widely
advertised, but we think it is safe to assume that any process that
examines every packet must run in \emph{constant} time and space,
subject to a hard realtime deadline.  This does not mean that
sophisticated techniques cannot be used at all; a simple, fast
algorithm could select TCP streams for further analysis by a more
sophisticated algorithm.  However, the sophisticated algorithm will be
unable to examine more than a tiny fraction of the total traffic, and
may not be able to trigger a response within the lifetime of a
connection.  These assumptions are consistent with the observed
behavior of the Great Firewall of China, whose keyword filters
could---perhaps still can---be fooled by placing a packet boundary in
just the right place, and whose active probes of suspected Tor bridges
occur minutes to hours after suspicion is aroused.

Within these constraints, what can be done?  All the classifiers
described above are too heavyweight to be practical as the first-stage
fast algorithm.  In the StegoTorus paper~\cite{ccs2012-stegotorus} we
describe a classifier that requires only a handful of arithmetic
operations per arriving packet, plus maintenance of a sliding-window
vector per stream under surveillance.  Briefly, the censor first
observes “typical” traffic via Tor to an objectionable website, and
models the probability distributions $\Pr[U_i]$, $\Pr[D_i]$ of the
cumulative sum of upstream (client-to-relay) and downstream
(relay-to-client) TCP payload lengths, over a sliding window of 250
packets.  When a stream is detected as suspicious (e.g.\ carrying Tor
traffic to somewhere), the censor begins maintaining a similar sliding
window of actual cumulative sums, $\{u_i\}$, $\{d_i\}$, for that
stream, and a running log-probability estimate
\begin{multline*}
\log \Pr\left[\text{objectionable site}\right]\\
= \sum_{i=1}^n \log \Pr[U_i=u_i] + \sum_{i=1}^n \log \Pr[D_i=d_i]
\end{multline*}
If the log-probability ever exceeds a threshold value, the stream is
classified as a visit to the objectionable site.  Maintaining the
running estimate requires only three memory lookups and two additions
for each incoming packet (log-probabilities eliminate the need for
multiplication).  Its cost could be reduced still further by using an
exponentially weighted moving average rather than a sliding window.
Despite its simplicity and cheapness, it achieved 98\% accuracy in a
basic experiment, in which we trained on 10 loads of the Facebook
front page, then tested on 20 more Facebook loads versus one load each
of the front page of 40 other websites from the Alexa top 100.

\begin{table}%
\centering\footnotesize
\parbox{245pt}{\caption{Comparison of the basic classification techniques discussed in this paper.}\label{t:comparison}}\par
\begin{minipage}{219.59134pt}
\def\th#1{\multicolumn{1}{c}{\textbf{#1}}}%
\def\footnoterule{\kern-7pt}
\begin{tabular}{llrr}%
\toprule
\th{Paper} & \th{Attack} & \th{Pages} & \th{Success} \\
\midrule
Liberatore & Naive Bayes     & 2000 & 70\% \\
           & Jaccard         & 2000 & 63\% \\
Cai        & Cai DL-SVM      &  100 & 85\% \\
           & Panchenko SVM   &  100 & 67\% \\
           & Herrmann Bayes  &  100 &  5\% \\
Weinberg   & Log-probability &   60 & 98\%\rlap{\footnote{\scriptsize\ Only distinguishes Facebook from not-Facebook.}} \\
\bottomrule
\end{tabular}%
\end{minipage}%
\end{table}

\subsection{Error Rates at Internet Scale}

Unlike the studies described above, a real-world censor must detect a
finite number of objectionable sites among an unbounded number of
uninteresting ones.  In this context, the classifier can fail to
detect a page as suspicious (false \emph{negative}), but it can also
fail to detect a page as innocent (false \emph{positive}).  The
classifier can be adjusted to produce either type of failure at a
desired rate, but this will have an approximately inverse effect on
the rate of the other---reducing the number of false negatives will
increase the number of false positives, and vice versa.

Unfortunately for the censor, they need an extremely low level of
false positives even in the first stage classifier, due to the sheer
size of the Internet. Plausible estimates for the total number of Web
sites (not pages) in the entire world, as of this writing, are just
shy of a billion.  There are probably only a few thousand
objectionable sites on the censor's blacklist.  A false positive rate
of 1\% in the first-stage classifier will mean that the second-stage
classifier has to field roughly a million false positives for every
genuine visit to an objectionable site.  If the censor wants only a
thousand false positives per objectionable site, they need a
first-stage false positive rate on the order of $10^{-6}$.  With any
of the classifiers discussed in this paper, a false positive rate that
low will produce a false \emph{negative} rate so high that the
censorship regime will not succeed in blocking the material it was
intended to block.

This order-of-magnitude analysis does not take into account \emph{how
  different} objectionable sites are from innocuous ones, how many
different sites (objectionable or otherwise) are actually visited by
users subject to various censorship regimes, or whether users of
circumvention systems are more likely to attempt to visit
objectionable sites. Getting a better idea of the true statistics is a
near-term research goal.  However, the literature to date has
considered attacks to be successful with true-positive rates only a
little better than chance.  Regardless of the underlying statistics,
we doubt that those attacks can be made into effective Internet-scale
censorship strategies.

\bibliography{review}
\end{document}
