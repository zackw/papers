\documentclass{zarticle}
%\usepackage{tikz}
%\usetikzlibrary{shapes.symbols,shapes.geometric,decorations.pathreplacing,calc}
\addbibresource{shared.bib}
\definecolor{todocolor}{RGB}{127,0,0}
\def\todo#1{{\color{todocolor}\bfseries [#1]}}
\def\needcite#1{\todo{cite: #1}}
\begin{document}

\title{The Representativeness of the Alexa $10^n$ for Fingerprinting Attacks}
\author{Zachary Weinberg}
\date{\today}
\maketitle

\begin{abstract}
\textit{Website fingerprinting} is a traffic-analytic attack which
aims to determine which websites, or perhaps even which specific
pages, a targeted individual is browsing, despite their use of an
“anonymity” service to conceal the ultimate destination of their
traffic.  A fingerprinting attacker loads each site of interest
themselves (using the same anonymity service), measures the network
traffic profile of each page load, and trains a machine classifier to
map traffic profiles back to sites.

To date, in the public literature, fingerprinting attacks have only
been attempted under laboratory conditions: only one targeted client,
no “background” network activity, and a tiny set of potential
destination pages, almost always the front pages of the “top
100--10,000 websites” taken from a longer list published by the Alexa
Corporation.  Despite plausible levels of success for a laboratory
test (classifier accuracies exceeding 90\% have been claimed), it is
unclear whether the attack is feasible “in the wild,” with orders of
magnitude more pages to discriminate. In this paper we assess the
representativeness of the Alexa list by comparing it to several other
public sources of big lists of URLs, \todo{summarize conclusions
  here}.
\end{abstract}

\section{Introduction}

secure channel vs unlinkable channel

between-site fingerprinting: an attack on unlinkability

related: within-site fingerprinting---attack on confidentiality

related: other attacks on unlinkability

HTTP background

important not to confuse: web sites, web pages, web resources, URLs

\section{The Central Role of Page Lists}

All between-site fingerprinting adversaries rely on a list of web
pages, selected in advance, which will be machine-classified somehow.
Alexa Internet, Inc.\ publishes an ordered list of the “top 1,000,000
websites,” updated daily, based on data collected from a “sample of
all internet users.”%~\cite{alexa.undated.topsites}
Most existing results on site fingerprinting (there are exceptions,
e.g.~\todo{citations}) take the first $N$ entries in this list as of
some date (choice of $N$ depending on the paper), and test one or more
classifiers on the base URL
(\textsf{http://www.\textit{site.domain}/}) of each; this is usually
described as being able to distinguish “the Alexa top $N$ sites”
without further qualification.

This practice is questionable for several reasons.  Most importantly,
it discards the distinction between a web \emph{page} and a web
\emph{site}.  A single site may host thousands of pages,
hierarchically organized, with radically different content on each;
the variation in content may or may not produce observable variation
in the traffic profile.  Several of Alexa's top 25 sites are blogging
and social networking services which host a range of content limited
only by human imagination and/or the terms of service.  It is
practically certain that any nontrivial exercise in “filtering” would
want to block some, but not all, of the content on
\textsf{wordpress.com}, for instance.

Alexa's list is not even a list of sites: it is more accurately
described as a list of \emph{registered domains}.  Wikipedia (the
organization) considers itself to publish \emph{many} encyclopedias,
one in each of the languages that has attracted a community of
authors; each language therefore gets its own subdomain.  A
nation-state adversary whose goal is to enforce political orthodoxy
may have no problem with the Wikipedia in the national language, but
frown upon foreign-language versions.  But Alexa lumps all of them
together as \textsf{wikipedia.org}, so researchers overlook the
distinction.

Even when a site is uniform in its content, the front page of any
given site is often \emph{not} representative of the “typical” page on
that site. Continuing with Wikipedia as an example, if you type
“\textsf{wikipedia.org}” into a browser's URL bar without further
qualification you will be automatically taken to
\url{https://www.wikipedia.org/}.  Compare this page to the
English-language front page, \url{https://en.wikipedia.org/}, and both
of them to a few actual encyclopedia articles,
e.g. \url{https://en.wikipedia.org/wiki/Ouachita_madtom}.  In each
case the overall page structure is radically different, and so is the
network traffic generated.

Beyond these concerns, Alexa does not document its own methodology, so
it is impossible to assess the accuracy of its ranking.  In
particular, they rely heavily on data from an unspecified set of
browser extensions which are instrumented to report site visits.
Without knowing which extensions these are and how widely installed
they are, we can't tell how representative their sample is.

\subsection{Comparing Lists of Pages}

We sought to determine directly how representative Alexa's “top $N$
site” list is of “real life” Web browsing activity, by comparing it to
other sources.  Raw lists of pages, sites, etc. were acquired from the
sources listed below, and then preprocessed as follows.

When (as with Alexa) the list discarded the URL scheme and any
subdomains, we reconstructed complete URLs by prepending
\verb|http://|, \verb|https://|, \verb|http://www.|, and
\verb|https://www.| to each, except when the domain name was
syntactically an IP address, in which case we only prepended the first
two.  When there was no path component to the URL (the norm for
Alexa), we appended a single \verb|/|; when a path was present
(sometimes the case for Alexa, far more often for other sources), we
preserved it, but also truncated it to a single \verb|/|.
\todo{[illustrative table here]}

We attempted to access each URL and follow HTTP redirects from it to a
“canonical” destination, using a program written for the purpose.  If
this attempt failed to contact an HTTP server, or the chain of
redirections did not culminate in a successful page retrieval
(i.e.\ HTTP “200 OK” status) the URL was discarded from further
consideration.  This step allows us to see through URL shortening
services (very common for links shared on social networks) and
eliminate URLs that no longer work or never worked (particularly
important for URLs reconstructed from domain names).  We did not
discard URLs where the final response was something other than HTML,
as these can legitimately be the target of an initial page load (for
instance, if someone is linked directly to a PDF document).

The checking program attempted to mimic the HTTP headers generated by
a Web browser, but it was not a complete browser.  Most importantly,
it did not interpret the contents of the HTTP response at all, which
will have caused us to miss redirections specified within the page
instead of via HTTP headers (there are several mechanisms for this).
It is also unable to access content that requires a login, which will
cause us to underestimate sites like Facebook and Quora where nearly
all content requires a login.

We present, for each list of URLs: the proportion that were
nonfunctional, functional HTTP, and functional HTTPS; the public
suffix\todo{cite} distribution of hostnames; the geographic
distribution of hostnames and servers; the distribution of path depth;
and make comparisons with the other lists. \todo{more?}

\subsection{Alexa}

Alexa makes a ranked list of 1,000,000 “sites,” updated daily,
available for free download.  As discussed above, this is more
properly understood as a list of \emph{registered domains}, although
it does include a few more specific entries, with a path below the
site root: roughly 1.5\% of the total, and none within the top 1000.

\subsection{Herdict: list of censored sites}

Herdict is one of several services that tracks deliberate blocking of
access to websites by nation-state adversaries.  It maintains a public
API that can be used to get a list of all sites reported as
inaccessible since 2009, and the countries where they were
inaccessible.  It relies on manual reports by end-users (via a browser
extension) and may therefore have systemic errors in both
directions---some classes of censorship may go unreported because they
are only seen by people who don't know about the service, and ordinary
site failures may be misreported as censorship.  It also only tracks
inaccessibility at the site level.  For instance, Youtube frequently
appears as blocked; we cannot tell whether a censorious agency
actually blocked the entire site or just specific videos they objected
to. Still, as a list of sites that probably have been blocked
somewhere, we think it makes a useful contrast.

\subsection{Twitter global sample}

Twitter is a globally popular “microblogging” service, allowing people
to post one short sentence at a time.  It is frequently used to share
links to other content of interest.  Twitter makes available a
realtime, continuously updated “small random sample” of all public
“tweets.” We captured every URL appearing in this sample stream during
the week beginning on 17 March 2014 at 04:00 UTC (midnight, US Eastern
Daylight Time).

Links posted to Twitter are often highly topical; this list reflects
world events at the time the sample was taken, such as \todo{...}.

\subsection{If time permits: DMoz}

DMoz, or the Open Directory Project, is a manually curated “directory
of the Web,” listing approximately 4.2 million sites.  Complete data
dumps are made available weekly.

\subsection{If time permits: Indiana U clickstream}

(access needs to be formally requested and then they mail you a hard drive)

\printbibliography

\end{document}
