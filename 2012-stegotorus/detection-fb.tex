\subsection{Identifying Visits to Facebook}\label{s:detect-fb}

Once the censor has identified TCP streams as Tor traffic, they would
also like to learn which sites are being accessed clandestinely.  We
present a simple method to determine whether a Tor user is visiting
Facebook; this site has sometimes been completely blocked by
government censors.  It is a cut-down version of Panchenko
et~al.~\cite{a-finger-onion}, which can identify accesses to a small
set of censored websites within a larger Tor session.  Their
classifier uses a support vector machine, which is too expensive to
run on a filtering router, even on a small number of streams. With
careful optimization, our classifier requires a handful of probability
calculations per arriving packet, plus maintenance of a sliding-window
vector per stream under surveillance; this should be acceptably cheap.

Once a stream has been identified as Tor traffic, the censor maintains
a pair of sequences, ${u_i}$ and ${d_i}$, sliding over the last $n$
non-empty packets observed by the filtering router.  Each $u_i$ is the
cumulative sum of payload lengths for packets $1$ through $i$ sent
“upward” (client to relay), and $d_i$ is the same for packets sent
“downward” (relay to client). The censor has previously observed
“typical” Facebook traffic, and modeled the probability distributions
$\Pr[U_i]$ and $\Pr[D_i]$ that one would expect to see if a trace were
a visit to Facebook.  Using this model, the censor computes
\begin{multline*}
\log \Pr\left[\{u_i\},\{d_i\} \text{ is Facebook}\right]\\
= \sum_{i=1}^n \log \Pr[U_i=u_i] + \sum_{i=1}^n \log \Pr[D_i=d_i]
\end{multline*}
Log-probabilities are used to avoid floating-point underflow, since
the per-packet probabilities can be very small.  If the overall
log-probability exceeds a threshold, the censor classifies the traffic
as a visit to Facebook.

\begin{figure}[h]
\centering
\input{figures/fb-detect}
\caption{Log-probabilities reported by Facebook classifier, shifted to
  place the classification threshold at zero on the y-axis. Visits to
  Facebook (squares) show the shifted log-probability for the first
  250 packets. Visits to non-Facebook sites (circles) show the maximum
  shifted log-probability observed for a 250-packet sliding
  window.}
\label{f:prob-facebook}
\end{figure}

We trained this classifier on the first $250$ packets transmitted in
each direction over ten visits to the Facebook home page (login
screen), and modeled the probability distributions as independent
Gaussians for each position in the sequence. This is a deliberate
departure from reality: $U_{i+1}$ has a strong dependence on $U_i$,
since they are cumulative sums, but treating them as independent makes
the classifier robust to variation in the order of resources
downloaded.  We then tested it on 20 more visits to Facebook, plus 40
visits to other web sites chosen from Alexa's categorized directory of
popular sites~\cite{d-alexa}.  For all of the test visits, we browsed
randomly until we had somewhere between 5,000 and 30,000 TCP packets;
this resulted in a total of over 450,000 total packets and 500MB of
Tor traffic. Figure~\ref{f:prob-facebook} shows the results.  Only one
of the Facebook visits is not detected, and none of the other sites
are misdetected as Facebook.

We augmented this attack to detect visits to nine of the top ten Alexa
sites.\footnote{\texttt{baidu.com} was excluded because visits to this
  site did not exchange enough packets to perform a meaningful
  analysis.} The classifier described above is intrinsically binary:
site-X or not-site-X.  An adversary wishing to know \emph{which} of
some set of sites was being visited would have to run a classifier for
each site in parallel, suffering additional resource costs
proportional to the number of sites.  If the adversary cares only
about visits to a fairly small number of sites, this will not be a
significant problem.

For each site, we trained a classifier using the same procedure as
described above for Facebook, using traces for ten visits to its front
page.  A traffic stream generated by a real user would not stop after
loading the front page of whatever site he or she was visiting.
Therefore, we adjusted the training window size for each site to
ensure that the classifier did not simply learn the overall amount of
data involved in loading the front page.  We then tested each binary
classifier on an additional ten visits to the target site, plus ten
traces for each of the other eight sites.

For test runs with “vanilla” Tor, we took the best classification
result obtained among four different window sizes: 50, 100, 150, and
200 packets.  For test runs with StegoTorus, we added a 500-packet
window, since StegoTorus-HTTP generates a much larger volume of
traffic.  We present classification accuracy in
Table~\ref{t:eval-auc}, as trapezoidally approximated AUC scores (area
under the receiver operating characteristic curve) for Tor and
StegoTorus visits to each of the nine sites.  AUC scores allow
evaluation of classifier effectiveness without first having to choose
a tradeoff between false negatives and false positives.

\begin{table}[ht]
\centering\footnotesize
\begin{tabular}{lrr}
\toprule
\multicolumn{1}{c}{\textbf{Web Site}} &
\multicolumn{1}{c}{\textbf{Tor}} &
\multicolumn{1}{c}{\textbf{StegoTorus}} \\
\midrule
Google          & 0.9697     & 0.6928 \\
Facebook        & 0.9441     & 0.5413 \\
Youtube         & 0.9947     & 0.4125 \\
Yahoo           & 0.8775     & 0.7400 \\
Wikipedia       & 0.9991     & 0.7716 \\
Windows Live    & 0.9403     & 0.6763 \\
Blogspot        & 0.9825     & 0.6209 \\
Amazon          & 0.9841     & 0.8684 \\
Twitter         & 0.9944     & 0.7366 \\
\bottomrule
\end{tabular}
\caption{AUC scores for detecting visits to nine of the Alexa top ten
  sites' front pages, over Tor and StegoTorus.}
\label{t:eval-auc}
\end{table}

%% This figure belongs to eval.tex but is coded here so LaTeX will put
%% it on the correct page.
\begin{figure*}
\centering
\input{figures/bnl}
\caption{Overhead and resilience of StegoTorus' \textit{HTTP}
  steganography module, compared to Tor alone and to StegoTorus in
  chopper-only operation.  Each data point shows median bandwidth
  consumption over a 20-second interval while transferring a
  continuous, fixed-rate stream of traffic over a 5.6~Mbps DSL link
  with adjustable latency.  Whiskers indicate the inter-hinge
  distance~\cite{s-boxplot}.  Tor and chopper-only StegoTorus do not
  suffer from latencies up to at least 450~ms.  The present
  \textit{HTTP} module is much more sensitive to latency, and cannot
  keep up with high-bandwidth streams at higher
  latencies.}\label{f:resilience}
\end{figure*}

An AUC score of 0.5 indicates a classifier performing no better than
random guessing, and a score of 1 indicates perfect accuracy.
Over Tor by itself, we can often obtain AUC scores better than 0.95,
but over StegoTorus, the scores drop to 0.75 or less in most cases.
For real-time classification of the traffic volume seen at a perimeter
router, the adversary requires an AUC score very close to 1 to avoid
being swamped by errors.  StegoTorus does not reduce the adversary's
odds all the way to chance, but it reduces them far enough to make the
attack impractical.

A determined adversary might train additional classifiers on visits to
sites of interest \emph{over StegoTorus}.  However, these classifiers
will be dependent on the covertext database that the adversary used
for training, so StegoTorus users who generate their own covertext
databases will be protected from this tactic.
